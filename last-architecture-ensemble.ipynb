{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12777248,"sourceType":"datasetVersion","datasetId":8077703}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:23:58.220438Z","iopub.execute_input":"2025-08-16T06:23:58.220928Z","iopub.status.idle":"2025-08-16T06:23:58.975906Z","shell.execute_reply.started":"2025-08-16T06:23:58.220900Z","shell.execute_reply":"2025-08-16T06:23:58.975283Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/intras-1/images.hdf5\n/kaggle/input/intras-1/best_vit_tab.keras\n/kaggle/input/intras-1/data_only_numerical.csv\n/kaggle/input/intras-1/EFFICIENT_NET.h5\n/kaggle/input/intras-1/subject_data.csv\n/kaggle/input/intras-1/RESNET_MODEL.h5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/intras-1/data_only_numerical.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:26:51.415817Z","iopub.execute_input":"2025-08-16T06:26:51.416864Z","iopub.status.idle":"2025-08-16T06:26:56.979679Z","shell.execute_reply.started":"2025-08-16T06:26:51.416828Z","shell.execute_reply":"2025-08-16T06:26:56.978804Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  target  age_approx  sex  clin_size_long_diam_mm   tbp_lv_A  \\\n0           0       0        60.0    0                    3.04  20.244422   \n1           1       0        60.0    0                    1.10  31.712570   \n2           2       0        60.0    0                    3.40  22.575830   \n3           3       0        65.0    0                    3.22  14.242329   \n4           4       0        55.0    0                    2.73  24.725520   \n\n   tbp_lv_Aext   tbp_lv_B  tbp_lv_Bext   tbp_lv_C  ...  tbp_lv_stdL  \\\n0    16.261975  26.922447    23.954773  33.684638  ...     2.036195   \n1    25.364740  26.331000    24.549290  41.219030  ...     0.853227   \n2    17.128170  37.970460    33.485410  44.174920  ...     1.743651   \n3    12.164757  21.448144    21.121356  25.746200  ...     1.258541   \n4    20.057470  26.464900    25.710460  36.217980  ...     2.085409   \n\n   tbp_lv_stdLExt  tbp_lv_symm_2axis  tbp_lv_symm_2axis_angle    tbp_lv_x  \\\n0        2.637780           0.590476                       85 -182.703552   \n1        3.912844           0.285714                       55   -0.078308   \n2        1.950777           0.361905                      105  123.649700   \n3        1.573733           0.209581                      130 -141.024780   \n4        2.480509           0.313433                       20  -72.315640   \n\n      tbp_lv_y    tbp_lv_z  iddx_full  iddx_1  tbp_lv_dnn_lesion_confidence  \n0   613.493652  -42.427948          0       0                     97.517282  \n1  1575.687000   57.174500          0       0                      3.141455  \n2  1472.010000  232.908900          0       0                     99.804040  \n3  1442.185791   58.359802          0       0                     99.989998  \n4  1488.720000   21.428960          0       0                     70.442510  \n\n[5 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>target</th>\n      <th>age_approx</th>\n      <th>sex</th>\n      <th>clin_size_long_diam_mm</th>\n      <th>tbp_lv_A</th>\n      <th>tbp_lv_Aext</th>\n      <th>tbp_lv_B</th>\n      <th>tbp_lv_Bext</th>\n      <th>tbp_lv_C</th>\n      <th>...</th>\n      <th>tbp_lv_stdL</th>\n      <th>tbp_lv_stdLExt</th>\n      <th>tbp_lv_symm_2axis</th>\n      <th>tbp_lv_symm_2axis_angle</th>\n      <th>tbp_lv_x</th>\n      <th>tbp_lv_y</th>\n      <th>tbp_lv_z</th>\n      <th>iddx_full</th>\n      <th>iddx_1</th>\n      <th>tbp_lv_dnn_lesion_confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>60.0</td>\n      <td>0</td>\n      <td>3.04</td>\n      <td>20.244422</td>\n      <td>16.261975</td>\n      <td>26.922447</td>\n      <td>23.954773</td>\n      <td>33.684638</td>\n      <td>...</td>\n      <td>2.036195</td>\n      <td>2.637780</td>\n      <td>0.590476</td>\n      <td>85</td>\n      <td>-182.703552</td>\n      <td>613.493652</td>\n      <td>-42.427948</td>\n      <td>0</td>\n      <td>0</td>\n      <td>97.517282</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>60.0</td>\n      <td>0</td>\n      <td>1.10</td>\n      <td>31.712570</td>\n      <td>25.364740</td>\n      <td>26.331000</td>\n      <td>24.549290</td>\n      <td>41.219030</td>\n      <td>...</td>\n      <td>0.853227</td>\n      <td>3.912844</td>\n      <td>0.285714</td>\n      <td>55</td>\n      <td>-0.078308</td>\n      <td>1575.687000</td>\n      <td>57.174500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.141455</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>60.0</td>\n      <td>0</td>\n      <td>3.40</td>\n      <td>22.575830</td>\n      <td>17.128170</td>\n      <td>37.970460</td>\n      <td>33.485410</td>\n      <td>44.174920</td>\n      <td>...</td>\n      <td>1.743651</td>\n      <td>1.950777</td>\n      <td>0.361905</td>\n      <td>105</td>\n      <td>123.649700</td>\n      <td>1472.010000</td>\n      <td>232.908900</td>\n      <td>0</td>\n      <td>0</td>\n      <td>99.804040</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>65.0</td>\n      <td>0</td>\n      <td>3.22</td>\n      <td>14.242329</td>\n      <td>12.164757</td>\n      <td>21.448144</td>\n      <td>21.121356</td>\n      <td>25.746200</td>\n      <td>...</td>\n      <td>1.258541</td>\n      <td>1.573733</td>\n      <td>0.209581</td>\n      <td>130</td>\n      <td>-141.024780</td>\n      <td>1442.185791</td>\n      <td>58.359802</td>\n      <td>0</td>\n      <td>0</td>\n      <td>99.989998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>55.0</td>\n      <td>0</td>\n      <td>2.73</td>\n      <td>24.725520</td>\n      <td>20.057470</td>\n      <td>26.464900</td>\n      <td>25.710460</td>\n      <td>36.217980</td>\n      <td>...</td>\n      <td>2.085409</td>\n      <td>2.480509</td>\n      <td>0.313433</td>\n      <td>20</td>\n      <td>-72.315640</td>\n      <td>1488.720000</td>\n      <td>21.428960</td>\n      <td>0</td>\n      <td>0</td>\n      <td>70.442510</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:26:56.981124Z","iopub.execute_input":"2025-08-16T06:26:56.981393Z","iopub.status.idle":"2025-08-16T06:26:56.987693Z","shell.execute_reply.started":"2025-08-16T06:26:56.981374Z","shell.execute_reply":"2025-08-16T06:26:56.986729Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'target', 'age_approx', 'sex', 'clin_size_long_diam_mm',\n       'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C',\n       'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext',\n       'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean',\n       'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n       'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n       'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n       'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n       'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n       'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'iddx_full', 'iddx_1',\n       'tbp_lv_dnn_lesion_confidence'],\n      dtype='object')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data = data.drop(['iddx_full','iddx_1','tbp_lv_dnn_lesion_confidence','tbp_lv_nevi_confidence'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:26:56.988647Z","iopub.execute_input":"2025-08-16T06:26:56.988895Z","iopub.status.idle":"2025-08-16T06:26:57.037657Z","shell.execute_reply.started":"2025-08-16T06:26:56.988869Z","shell.execute_reply":"2025-08-16T06:26:57.036989Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data.drop(columns=[\"Unnamed: 0\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:26:57.038848Z","iopub.execute_input":"2025-08-16T06:26:57.039127Z","iopub.status.idle":"2025-08-16T06:26:57.079379Z","shell.execute_reply.started":"2025-08-16T06:26:57.039110Z","shell.execute_reply":"2025-08-16T06:26:57.078800Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"data_1 = pd.read_csv(\"/kaggle/input/intras-1/subject_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:26:57.080038Z","iopub.execute_input":"2025-08-16T06:26:57.080216Z","iopub.status.idle":"2025-08-16T06:27:05.917239Z","shell.execute_reply.started":"2025-08-16T06:26:57.080202Z","shell.execute_reply":"2025-08-16T06:27:05.916626Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_629/232299192.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n  data_1 = pd.read_csv(\"/kaggle/input/intras-1/subject_data.csv\")\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import h5py\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport io\nimport numpy as np\n\nfile_path = \"/kaggle/input/intras-1/images.hdf5\"\n\n# Filter data to get ISIC IDs where the target is 1\nisic_codes = data_1[data_1[\"target\"] >= 0][' \"isic_id\"'].tolist()\nimages = []\ntarget = data_1['target'].values\nwith h5py.File(file_path, \"r\") as hdf:\n    ls = list(hdf.keys())  # list all HDF5 keys\n    for i in isic_codes:\n        image_data = hdf[i]\n        # Check if the dataset is scalar and contains bytes\n        if image_data.shape == () and image_data.dtype.kind == 'S':\n            # Read bytes and open as PIL image\n            image_bytes = image_data[()]\n            img_pil = Image.open(io.BytesIO(image_bytes))\n            images.append(img_pil)\n        else:\n            print(f\"Item '{i}' is not a scalar byte string\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:27:05.919073Z","iopub.execute_input":"2025-08-16T06:27:05.919289Z","iopub.status.idle":"2025-08-16T06:30:28.595037Z","shell.execute_reply.started":"2025-08-16T06:27:05.919273Z","shell.execute_reply":"2025-08-16T06:30:28.594423Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"len(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:30:41.204687Z","iopub.execute_input":"2025-08-16T06:30:41.205189Z","iopub.status.idle":"2025-08-16T06:30:41.219251Z","shell.execute_reply.started":"2025-08-16T06:30:41.205162Z","shell.execute_reply":"2025-08-16T06:30:41.218681Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"401059"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data['images_idx'] = [i for i in range(0, len(target))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:30:42.957930Z","iopub.execute_input":"2025-08-16T06:30:42.958713Z","iopub.status.idle":"2025-08-16T06:30:43.077712Z","shell.execute_reply.started":"2025-08-16T06:30:42.958687Z","shell.execute_reply":"2025-08-16T06:30:43.077086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"mask = data['target'] == 1\ndf_ones = data[mask].copy()\ndf_zeros = data[~mask].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:30:44.833901Z","iopub.execute_input":"2025-08-16T06:30:44.834208Z","iopub.status.idle":"2025-08-16T06:30:44.938412Z","shell.execute_reply.started":"2025-08-16T06:30:44.834189Z","shell.execute_reply":"2025-08-16T06:30:44.937418Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_ones, X_test_ones, y_train_ones, y_test_ones = train_test_split(df_ones.drop(columns=['target']), df_ones['target'], test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:30:47.621428Z","iopub.execute_input":"2025-08-16T06:30:47.622146Z","iopub.status.idle":"2025-08-16T06:30:48.732859Z","shell.execute_reply.started":"2025-08-16T06:30:47.622119Z","shell.execute_reply":"2025-08-16T06:30:48.732325Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sampled_x_train_zeros, sampled_x_zeros, sampled_y_train_zeros, samples_y_zeros = train_test_split(df_zeros.drop(columns=['target']), df_zeros['target'], test_size=0.01, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:30:59.450776Z","iopub.execute_input":"2025-08-16T06:30:59.451659Z","iopub.status.idle":"2025-08-16T06:30:59.600530Z","shell.execute_reply.started":"2025-08-16T06:30:59.451634Z","shell.execute_reply":"2025-08-16T06:30:59.599897Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X_TR_ZEROS, X_TEST_ZEROS, Y_TR_ZEROS, Y_TEST_ZEROS = train_test_split(sampled_x_zeros, samples_y_zeros, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:00.698801Z","iopub.execute_input":"2025-08-16T06:31:00.699446Z","iopub.status.idle":"2025-08-16T06:31:00.705227Z","shell.execute_reply.started":"2025-08-16T06:31:00.699419Z","shell.execute_reply":"2025-08-16T06:31:00.704672Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Features (X)\nX_train = pd.concat([X_train_ones, X_TR_ZEROS], ignore_index=True)\nX_test  = pd.concat([X_test_ones, X_TEST_ZEROS], ignore_index=True)\n\n# Labels (y) - keep as DataFrame, not array\ny_train = pd.concat([y_train_ones, Y_TR_ZEROS], ignore_index=True)\ny_test  = pd.concat([y_test_ones, Y_TEST_ZEROS], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:09.908640Z","iopub.execute_input":"2025-08-16T06:31:09.909360Z","iopub.status.idle":"2025-08-16T06:31:09.915903Z","shell.execute_reply.started":"2025-08-16T06:31:09.909335Z","shell.execute_reply":"2025-08-16T06:31:09.915358Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\n\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()\ny_train = torch.from_numpy(y_train).float()\ny_test = torch.from_numpy(y_test).float()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:13.435088Z","iopub.execute_input":"2025-08-16T06:31:13.435758Z","iopub.status.idle":"2025-08-16T06:31:18.251041Z","shell.execute_reply.started":"2025-08-16T06:31:13.435734Z","shell.execute_reply":"2025-08-16T06:31:18.250102Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:18.252356Z","iopub.execute_input":"2025-08-16T06:31:18.252835Z","iopub.status.idle":"2025-08-16T06:31:18.257318Z","shell.execute_reply.started":"2025-08-16T06:31:18.252810Z","shell.execute_reply":"2025-08-16T06:31:18.256739Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(920, 35)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"images_test = []\nimages_train = []\nfor i in X_test['images_idx'].to_numpy():\n    images_test.append(images[i])\nfor i in X_train['images_idx'].to_numpy():\n    images_train.append(images[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:18.257881Z","iopub.execute_input":"2025-08-16T06:31:18.258155Z","iopub.status.idle":"2025-08-16T06:31:18.275608Z","shell.execute_reply.started":"2025-08-16T06:31:18.258137Z","shell.execute_reply":"2025-08-16T06:31:18.274903Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from tensorflow import keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:31:36.542658Z","iopub.execute_input":"2025-08-16T06:31:36.543298Z","iopub.status.idle":"2025-08-16T06:31:42.867980Z","shell.execute_reply.started":"2025-08-16T06:31:36.543270Z","shell.execute_reply":"2025-08-16T06:31:42.867186Z"}},"outputs":[{"name":"stderr","text":"2025-08-16 06:31:37.958256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755325898.143222     629 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755325898.195890     629 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.Resize(128),\n    transforms.ToTensor()\n])\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, pil_images, targets, X, transform=None):\n        self.X = X\n        self.images = pil_images\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n        label = self.targets[idx]\n        return image, self.X.iloc[idx].values, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:02.218137Z","iopub.execute_input":"2025-08-16T06:32:02.219174Z","iopub.status.idle":"2025-08-16T06:32:04.066125Z","shell.execute_reply.started":"2025-08-16T06:32:02.219146Z","shell.execute_reply":"2025-08-16T06:32:04.065319Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"training = CustomDataset(images_train, y_train, X_train.drop(columns=['images_idx']), preprocess)\ntesting = CustomDataset(images_test, y_test, X_test.drop(columns=['images_idx']), preprocess)\ntrain_dataloader = DataLoader(training, batch_size=128, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(testing, batch_size=128, shuffle=True, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:07.932368Z","iopub.execute_input":"2025-08-16T06:32:07.933221Z","iopub.status.idle":"2025-08-16T06:32:07.939480Z","shell.execute_reply.started":"2025-08-16T06:32:07.933195Z","shell.execute_reply":"2025-08-16T06:32:07.938881Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def dataloader_to_numpy(dataloader):\n    images, X, y = [], [], []\n    for imgs, x, labels in dataloader:\n        imgs = imgs.permute(0, 2, 3, 1)  # (B, H, W, C)\n        images.append(imgs)\n        X.append(x)\n        y.append(labels)\n    return np.concatenate(images), np.concatenate(y), np.concatenate(X)\n\nX_train_img, y_train_, X_train_tab = dataloader_to_numpy(train_dataloader)\nX_test_img, y_test_, X_test_tab = dataloader_to_numpy(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:18.014222Z","iopub.execute_input":"2025-08-16T06:32:18.014539Z","iopub.status.idle":"2025-08-16T06:32:23.198587Z","shell.execute_reply.started":"2025-08-16T06:32:18.014520Z","shell.execute_reply":"2025-08-16T06:32:23.197949Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:23.199804Z","iopub.execute_input":"2025-08-16T06:32:23.200095Z","iopub.status.idle":"2025-08-16T06:32:23.203768Z","shell.execute_reply.started":"2025-08-16T06:32:23.200070Z","shell.execute_reply":"2025-08-16T06:32:23.203077Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def dataloader_to_numpy(dataloader):\n    images, X, y = [], [], []\n    for imgs, x, labels in dataloader:\n        imgs = imgs.permute(0, 2, 3, 1)  # (B, H, W, C)\n        images.append(imgs)\n        X.append(x)\n        y.append(labels)\n    return np.concatenate(images), np.concatenate(y), np.concatenate(X)\n\nX_train_img, y_train_, X_train_tab_ = dataloader_to_numpy(train_dataloader)\nX_test_img, y_test_, X_test_tab_ = dataloader_to_numpy(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:28.498338Z","iopub.execute_input":"2025-08-16T06:32:28.498643Z","iopub.status.idle":"2025-08-16T06:32:31.741388Z","shell.execute_reply.started":"2025-08-16T06:32:28.498620Z","shell.execute_reply":"2025-08-16T06:32:31.740744Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"seed_value = 42\ntf.random.set_seed(seed_value)   \nnp.random.seed(seed_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:24:11.009042Z","iopub.execute_input":"2025-08-16T05:24:11.009374Z","iopub.status.idle":"2025-08-16T05:24:11.014842Z","shell.execute_reply.started":"2025-08-16T05:24:11.009348Z","shell.execute_reply":"2025-08-16T05:24:11.013866Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor()\n])\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, pil_images, targets, X, transform=None):\n        self.X = X\n        self.images = pil_images\n        self.targets = targets\n        self.transform = transform\n        assert len(self.images) == len(self.X) == len(self.targets), \"Length mismatch\"\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n        label = self.targets[idx]\n        return image, self.X.iloc[idx].values, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:48.258228Z","iopub.execute_input":"2025-08-16T06:32:48.258552Z","iopub.status.idle":"2025-08-16T06:32:48.264875Z","shell.execute_reply.started":"2025-08-16T06:32:48.258528Z","shell.execute_reply":"2025-08-16T06:32:48.264020Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"ones_index = list(X_train_ones['images_idx'])\n# print(ones_index)\n\nimages_train_ones  = [images[i] for i in ones_index]\n# print(images_train_ones)\n\nprint(len(images_train_ones))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:37.989150Z","iopub.execute_input":"2025-08-16T06:32:37.989445Z","iopub.status.idle":"2025-08-16T06:32:37.994492Z","shell.execute_reply.started":"2025-08-16T06:32:37.989424Z","shell.execute_reply":"2025-08-16T06:32:37.993691Z"}},"outputs":[{"name":"stdout","text":"275\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n\n# Initialize your ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\naugmented_images_train_ones = []\naugmented_ones_train= []\nk = 0\nfor pil_img in images_train_ones:\n    x = img_to_array(pil_img)\n    x = x.reshape((1,) + x.shape)  # reshape for datagen\n    i = 0\n    for batch in datagen.flow(x, batch_size=1):\n        augmented_images_train_ones.append(batch[0].astype(np.uint8))  # Store augmented image array\n        i += 1\n        augmented_ones_train.append(X_train_ones.iloc[k].values)\n        if i >= 5:  # Number of augmentations per image\n            break\n    k+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:39.184086Z","iopub.execute_input":"2025-08-16T06:32:39.184350Z","iopub.status.idle":"2025-08-16T06:32:44.078907Z","shell.execute_reply.started":"2025-08-16T06:32:39.184330Z","shell.execute_reply":"2025-08-16T06:32:44.077906Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"augmented_ones_df = pd.DataFrame(augmented_ones_train, columns = X_train_ones.columns)\n\nX_train_new = pd.concat([X_train, augmented_ones_df], ignore_index=True)\n\nimages_train_new = images_train.copy()\n\naugmented_images_train_ones_pil = [\n    Image.fromarray(arr).convert('RGB') for arr in augmented_images_train_ones\n]\n\nfor img in augmented_images_train_ones_pil:\n    images_train_new.append(img)\n\n\ny_train_new = [i for i in y_train]\n\nfor j in range(0, len(augmented_images_train_ones)):\n    y_train_new.append(1)\n\ny_train_new = np.array([val for val in y_train_new]).flatten()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:32:44.080169Z","iopub.execute_input":"2025-08-16T06:32:44.080419Z","iopub.status.idle":"2025-08-16T06:32:44.184036Z","shell.execute_reply.started":"2025-08-16T06:32:44.080402Z","shell.execute_reply":"2025-08-16T06:32:44.183440Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"training = CustomDataset(images_train_new, y_train_new, X_train_new.drop(columns=['images_idx']), preprocess)\ntesting = CustomDataset(images_test, y_test, X_test.drop(columns=['images_idx']), preprocess)\ntrain_dataloader = DataLoader(training, batch_size=128, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(testing, batch_size=128, shuffle=True, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:46:54.496862Z","iopub.execute_input":"2025-08-16T05:46:54.497098Z","iopub.status.idle":"2025-08-16T05:46:54.503568Z","shell.execute_reply.started":"2025-08-16T05:46:54.497081Z","shell.execute_reply":"2025-08-16T05:46:54.502994Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def dataloader_to_numpy(dataloader):\n    images, X, y = [], [], []\n    for imgs, x, labels in dataloader:\n        imgs = imgs.permute(0, 2, 3, 1)  # (B, H, W, C)\n        images.append(imgs)\n        X.append(x)\n        y.append(labels)\n    return np.concatenate(images), np.concatenate(y), np.concatenate(X)\n\nX_train_img, y_train_, X_train_tab_ = dataloader_to_numpy(train_dataloader)\nX_test_img, y_test_, X_test_tab_ = dataloader_to_numpy(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:46:54.504613Z","iopub.execute_input":"2025-08-16T05:46:54.504798Z","iopub.status.idle":"2025-08-16T05:47:07.009678Z","shell.execute_reply.started":"2025-08-16T05:46:54.504776Z","shell.execute_reply":"2025-08-16T05:47:07.009128Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"X_train_img[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:47:07.010397Z","iopub.execute_input":"2025-08-16T05:47:07.010609Z","iopub.status.idle":"2025-08-16T05:47:07.015740Z","shell.execute_reply.started":"2025-08-16T05:47:07.010592Z","shell.execute_reply":"2025-08-16T05:47:07.015088Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(224, 224, 3)"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"# VIT","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score\n\nprint(\"TF:\", tf.__version__)\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Image / model params\nIMG_SIZE = (224, 224, 3)\nPATCH_SIZE = 16                 # 224 / 16 => 14x14 = 196 tokens\nNUM_PATCHES = (IMG_SIZE[0] // PATCH_SIZE) * (IMG_SIZE[1] // PATCH_SIZE)\nEMBED_DIM = 256\nNUM_HEADS = 8\nMLP_DIM = 512\nENCODER_DEPTH = 8\nDROP_RATE = 0.1\n\n# Tabular params\nTAB_DIM = 34                    # you said 34 columns\n\n# Training\nBATCH_SIZE = 32\nEPOCHS = 20\nLR = 3e-4\nSEED = 42\n\n# Mixed precision (optional, speed-up on GPU)\nif tf.config.list_physical_devices(\"GPU\"):\n    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n    print(\"Using mixed precision\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:23.183208Z","iopub.execute_input":"2025-08-16T06:33:23.183997Z","iopub.status.idle":"2025-08-16T06:33:23.207106Z","shell.execute_reply.started":"2025-08-16T06:33:23.183968Z","shell.execute_reply":"2025-08-16T06:33:23.206473Z"}},"outputs":[{"name":"stdout","text":"TF: 2.18.0\nUsing mixed precision\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def normalize_images(x):\n    x = tf.convert_to_tensor(x)\n    x = tf.image.convert_image_dtype(x, tf.float32)  # [0,1]\n    return x\n\nX_train_img_tf = normalize_images(X_train_img)\nX_test_img_tf  = normalize_images(X_test_img)\n\n# 2.2 Scale tabular with StandardScaler (fit on train only)\nscaler = StandardScaler()\nX_train_tab_scaled = scaler.fit_transform(X_train_tab_).astype(\"float32\")\nX_test_tab_scaled = scaler.transform(X_test_tab_).astype(\"float32\")\n\n# 2.3 Build tf.data datasets\ndef make_ds(imgs, tabs, ys, training: bool):\n    ds_imgs = tf.data.Dataset.from_tensor_slices(imgs)\n    ds_tabs = tf.data.Dataset.from_tensor_slices(tabs)\n    ds_ys   = tf.data.Dataset.from_tensor_slices(ys.astype(\"float32\"))\n    ds = tf.data.Dataset.zip((ds_imgs, ds_tabs, ds_ys))\n\n    def to_dict(img, tab, y):\n        return {\"image\": img, \"tab\": tab}, y\n\n    ds = ds.map(to_dict, num_parallel_calls=AUTOTUNE)\n    if training:\n        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    return ds\n\ntrain_ds = make_ds(X_train_img_tf, X_train_tab_scaled, y_train_, training=True)\nval_ds   = make_ds(X_test_img_tf,  X_test_tab_scaled, y_test_,  training=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:37:41.901490Z","iopub.execute_input":"2025-08-16T06:37:41.902151Z","iopub.status.idle":"2025-08-16T06:37:45.562965Z","shell.execute_reply.started":"2025-08-16T06:37:41.902124Z","shell.execute_reply":"2025-08-16T06:37:45.562403Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class PatchExtractor(layers.Layer):\n    \"\"\"Extract non-overlapping patches using a Conv2D.\"\"\"\n    def __init__(self, patch_size=PATCH_SIZE, embed_dim=EMBED_DIM, **kwargs):\n        super().__init__(**kwargs)\n        self.proj = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, padding=\"valid\")\n\n    def call(self, x):\n        # x: [B, H, W, C] -> conv -> [B, H/P, W/P, EMBED]\n        x = self.proj(x)\n        # [B, h, w, EMBED] -> [B, h*w, EMBED]\n        x = tf.reshape(x, [tf.shape(x)[0], -1, tf.shape(x)[-1]])\n        return x  # tokens\n\nclass AddClassTokenAndPos(layers.Layer):\n    def __init__(self, num_patches, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.cls = self.add_weight(\n            name=\"cls\",\n            shape=(1, 1, embed_dim),\n            initializer=\"zeros\",\n            trainable=True\n        )\n        self.pos = self.add_weight(\n            name=\"pos\",\n            shape=(1, num_patches + 1, embed_dim),\n            initializer=\"random_normal\",\n            trainable=True\n        )\n\n    def call(self, x):\n        batch_size = tf.shape(x)[0]\n        cls_tokens = tf.broadcast_to(self.cls, [batch_size, 1, tf.shape(x)[-1]])\n        x = tf.concat([cls_tokens, x], axis=1)\n        return x + self.pos\n\ndef transformer_encoder(x, num_heads=NUM_HEADS, mlp_dim=MLP_DIM, drop=DROP_RATE):\n    # PreNorm -> MHA -> Drop -> Residual\n    y = layers.LayerNormalization(epsilon=1e-6)(x)\n    y = layers.MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(y, y)\n    y = layers.Dropout(drop)(y)\n    x = layers.Add()([x, y])\n\n    # PreNorm -> MLP -> Drop -> Residual\n    y = layers.LayerNormalization(epsilon=1e-6)(x)\n    y = layers.Dense(mlp_dim, activation=\"gelu\")(y)\n    y = layers.Dropout(drop)(y)\n    y = layers.Dense(x.shape[-1])(y)\n    y = layers.Dropout(drop)(y)\n    return layers.Add()([x, y])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:28.478679Z","iopub.execute_input":"2025-08-16T06:33:28.478955Z","iopub.status.idle":"2025-08-16T06:33:28.487644Z","shell.execute_reply.started":"2025-08-16T06:33:28.478913Z","shell.execute_reply":"2025-08-16T06:33:28.487091Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.Resize(128),\n    transforms.ToTensor()\n])\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, pil_images, targets, X, transform=None):\n        self.X = X\n        self.images = pil_images\n        self.targets = targets\n        self.transform = transform\n        assert len(self.images) == len(self.X) == len(self.targets), \"Length mismatch\"\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n        label = self.targets[idx]\n        return image, self.X.iloc[idx].values, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:37:24.144968Z","iopub.execute_input":"2025-08-16T06:37:24.145262Z","iopub.status.idle":"2025-08-16T06:37:24.151149Z","shell.execute_reply.started":"2025-08-16T06:37:24.145243Z","shell.execute_reply":"2025-08-16T06:37:24.150361Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"training = CustomDataset(images_train_new, y_train_new, X_train_new.drop(columns=['images_idx']), preprocess)\ntesting = CustomDataset(images_test, y_test, X_test.drop(columns=['images_idx']), preprocess)\ntrain_dataloader = DataLoader(training, batch_size=128, shuffle=True, pin_memory=True)\ntest_dataloader = DataLoader(testing, batch_size=128, shuffle=True, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:37:25.924317Z","iopub.execute_input":"2025-08-16T06:37:25.924598Z","iopub.status.idle":"2025-08-16T06:37:25.931573Z","shell.execute_reply.started":"2025-08-16T06:37:25.924578Z","shell.execute_reply":"2025-08-16T06:37:25.930911Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def dataloader_to_numpy(dataloader):\n    images, X, y = [], [], []\n    for imgs, x, labels in dataloader:\n        imgs = imgs.permute(0, 2, 3, 1)  # (B, H, W, C)\n        images.append(imgs.numpy())\n        X.append(x.numpy())\n        y.append(labels.numpy())\n    return np.concatenate(images), np.concatenate(y), np.concatenate(X)\n\nX_train_img, y_train_, X_train_tab_ = dataloader_to_numpy(train_dataloader)\nX_test_img, y_test_, X_test_tab_ = dataloader_to_numpy(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:37:26.621110Z","iopub.execute_input":"2025-08-16T06:37:26.621982Z","iopub.status.idle":"2025-08-16T06:37:30.833481Z","shell.execute_reply.started":"2025-08-16T06:37:26.621926Z","shell.execute_reply":"2025-08-16T06:37:30.832562Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"\ndef build_model():\n    # Image branch (ViT)\n    img_in = layers.Input(shape=IMG_SIZE, name=\"image\")\n    x = PatchExtractor(PATCH_SIZE, EMBED_DIM)(img_in)\n    x = AddClassTokenAndPos(NUM_PATCHES, EMBED_DIM)(x)\n    for _ in range(ENCODER_DEPTH):\n        x = transformer_encoder(x, NUM_HEADS, MLP_DIM, DROP_RATE)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    cls_token = x[:, 0]                        # [B, D]\n    img_feat = layers.Dropout(DROP_RATE, name=\"image_features\")(cls_token)\n\n    # Tabular branch (simple MLP)\n    tab_in = layers.Input(shape=(TAB_DIM,), name=\"tab\")\n    t = layers.Dense(128, activation=\"relu\")(tab_in)\n    t = layers.BatchNormalization()(t)\n    t = layers.Dropout(0.2)(t)\n    t = layers.Dense(128, activation=\"relu\")(t)\n    t = layers.BatchNormalization()(t)\n    t = layers.Dropout(0.2)(t)\n\n    # Fuse\n    fused = layers.Concatenate(name=\"fusion\")([img_feat, t])\n    z = layers.Dense(256, activation=\"relu\")(fused)\n    z = layers.BatchNormalization()(z)\n    z = layers.Dropout(0.3)(z)\n\n    # Binary output (ensure float32 to avoid mixed precision issues on loss/metrics)\n    out = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"pred\")(z)\n\n    return keras.Model(inputs={\"image\": img_in, \"tab\": tab_in}, outputs=out, name=\"ViT_Tab_Fusion\")\n\nmodel = build_model()\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:31.466067Z","iopub.execute_input":"2025-08-16T06:33:31.466371Z","iopub.status.idle":"2025-08-16T06:33:32.873932Z","shell.execute_reply.started":"2025-08-16T06:33:31.466348Z","shell.execute_reply":"2025-08-16T06:33:32.873317Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"ViT_Tab_Fusion\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ViT_Tab_Fusion\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast (\u001b[38;5;33mCast\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ patch_extractor     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m196,864\u001b[0m │ cast[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mPatchExtractor\u001b[0m)    │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_class_token_an… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m50,688\u001b[0m │ patch_extractor[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mAddClassTokenAndP…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_class_token_… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_class_token_… │\n│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_11 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_13 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m2,103,552\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_14 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ tab (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_1 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ tab[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m131,328\u001b[0m │ dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m4,480\u001b[0m │ cast_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_15 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item (\u001b[38;5;33mGetItem\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ fusion              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ image_features[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m98,560\u001b[0m │ fusion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pred (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ patch_extractor     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ cast[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchExtractor</span>)    │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_class_token_an… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">50,688</span> │ patch_extractor[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AddClassTokenAndP…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_class_token_… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_class_token_… │\n│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,103,552</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ tab (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tab[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> │ cast_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ image_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ fusion              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pred (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,309,825\u001b[0m (73.66 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,309,825</span> (73.66 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,308,801\u001b[0m (73.66 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,308,801</span> (73.66 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"class F1AtThreshold(keras.metrics.Metric):\n    \"\"\"Compute F1 with fixed threshold (default 0.5).\"\"\"\n    def __init__(self, threshold=0.5, name=\"f1\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.threshold = threshold\n        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n        y_pred = tf.cast(tf.reshape(y_pred, [-1]) >= self.threshold, tf.float32)\n\n        tp = tf.reduce_sum(y_pred * y_true)\n        fp = tf.reduce_sum(y_pred * (1.0 - y_true))\n        fn = tf.reduce_sum((1.0 - y_pred) * y_true)\n\n        self.tp.assign_add(tp)\n        self.fp.assign_add(fp)\n        self.fn.assign_add(fn)\n\n    def result(self):\n        precision = self.tp / (self.tp + self.fp + 1e-7)\n        recall = self.tp / (self.tp + self.fn + 1e-7)\n        f1 = 2.0 * precision * recall / (precision + recall + 1e-7)\n        return f1\n\n    def reset_states(self):\n        for v in self.variables:\n            v.assign(0.0)\n\nmetrics = [\n    keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n    keras.metrics.Recall(name=\"recall\",     thresholds=0.5),\n    keras.metrics.AUC(name=\"auc_roc\", curve=\"ROC\"),\n    F1AtThreshold(threshold=0.5, name=\"f1\"),\n]\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(LR),\n    loss=\"binary_crossentropy\",\n    metrics=metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:34.098342Z","iopub.execute_input":"2025-08-16T06:33:34.098631Z","iopub.status.idle":"2025-08-16T06:33:34.138987Z","shell.execute_reply.started":"2025-08-16T06:33:34.098612Z","shell.execute_reply":"2025-08-16T06:33:34.138422Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"ckpt_path = \"vit_image.keras\"\ncbs = [\n    callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_auc_roc\", mode=\"max\",\n                              save_best_only=True, verbose=1),\n    callbacks.EarlyStopping(monitor=\"val_auc_roc\", mode=\"max\",\n                            patience=5, restore_best_weights=True),\n    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n]\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,   # per your request, using test as validation\n    epochs=EPOCHS,\n    callbacks=cbs,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:00:15.875577Z","iopub.execute_input":"2025-08-16T06:00:15.875850Z","iopub.status.idle":"2025-08-16T06:09:06.639258Z","shell.execute_reply.started":"2025-08-16T06:00:15.875828Z","shell.execute_reply":"2025-08-16T06:09:06.638694Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - auc_roc: 0.7699 - f1: 0.6228 - loss: 0.6182 - precision: 0.5660 - recall: 0.6956\nEpoch 1: val_auc_roc improved from -inf to 0.87410, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 432ms/step - auc_roc: 0.7703 - f1: 0.6232 - loss: 0.6176 - precision: 0.5665 - recall: 0.6958 - val_auc_roc: 0.8741 - val_f1: 0.5027 - val_loss: 0.2657 - val_precision: 0.7077 - val_recall: 0.3898 - learning_rate: 3.0000e-04\nEpoch 2/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - auc_roc: 0.8798 - f1: 0.7225 - loss: 0.4364 - precision: 0.7032 - recall: 0.7446\nEpoch 2: val_auc_roc improved from 0.87410 to 0.88343, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 201ms/step - auc_roc: 0.8798 - f1: 0.7226 - loss: 0.4363 - precision: 0.7034 - recall: 0.7446 - val_auc_roc: 0.8834 - val_f1: 0.5859 - val_loss: 0.3878 - val_precision: 0.4860 - val_recall: 0.7373 - learning_rate: 3.0000e-04\nEpoch 3/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - auc_roc: 0.9065 - f1: 0.7711 - loss: 0.3797 - precision: 0.7647 - recall: 0.7787\nEpoch 3: val_auc_roc improved from 0.88343 to 0.88928, saving model to vit_image.keras\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 198ms/step - auc_roc: 0.9065 - f1: 0.7711 - loss: 0.3796 - precision: 0.7648 - recall: 0.7787 - val_auc_roc: 0.8893 - val_f1: 0.6227 - val_loss: 0.3256 - val_precision: 0.5484 - val_recall: 0.7203 - learning_rate: 3.0000e-04\nEpoch 4/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc_roc: 0.9188 - f1: 0.7825 - loss: 0.3531 - precision: 0.8019 - recall: 0.7643\nEpoch 4: val_auc_roc improved from 0.88928 to 0.89554, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 195ms/step - auc_roc: 0.9188 - f1: 0.7825 - loss: 0.3530 - precision: 0.8019 - recall: 0.7643 - val_auc_roc: 0.8955 - val_f1: 0.6245 - val_loss: 0.2642 - val_precision: 0.6218 - val_recall: 0.6271 - learning_rate: 1.5000e-04\nEpoch 5/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - auc_roc: 0.9286 - f1: 0.7899 - loss: 0.3356 - precision: 0.7939 - recall: 0.7872\nEpoch 5: val_auc_roc improved from 0.89554 to 0.90566, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 198ms/step - auc_roc: 0.9286 - f1: 0.7899 - loss: 0.3356 - precision: 0.7939 - recall: 0.7872 - val_auc_roc: 0.9057 - val_f1: 0.6464 - val_loss: 0.2865 - val_precision: 0.5862 - val_recall: 0.7203 - learning_rate: 1.5000e-04\nEpoch 6/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - auc_roc: 0.9363 - f1: 0.8149 - loss: 0.3103 - precision: 0.8254 - recall: 0.8048\nEpoch 6: val_auc_roc did not improve from 0.90566\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 181ms/step - auc_roc: 0.9363 - f1: 0.8148 - loss: 0.3103 - precision: 0.8253 - recall: 0.8048 - val_auc_roc: 0.9021 - val_f1: 0.6564 - val_loss: 0.2754 - val_precision: 0.6028 - val_recall: 0.7203 - learning_rate: 1.5000e-04\nEpoch 7/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9351 - f1: 0.8065 - loss: 0.3167 - precision: 0.8284 - recall: 0.7858\nEpoch 7: val_auc_roc did not improve from 0.90566\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 181ms/step - auc_roc: 0.9351 - f1: 0.8064 - loss: 0.3167 - precision: 0.8283 - recall: 0.7858 - val_auc_roc: 0.9036 - val_f1: 0.6420 - val_loss: 0.2506 - val_precision: 0.6240 - val_recall: 0.6610 - learning_rate: 7.5000e-05\nEpoch 8/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - auc_roc: 0.9382 - f1: 0.8169 - loss: 0.3071 - precision: 0.8331 - recall: 0.8017\nEpoch 8: val_auc_roc improved from 0.90566 to 0.90757, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 195ms/step - auc_roc: 0.9382 - f1: 0.8169 - loss: 0.3071 - precision: 0.8330 - recall: 0.8017 - val_auc_roc: 0.9076 - val_f1: 0.6667 - val_loss: 0.2654 - val_precision: 0.6143 - val_recall: 0.7288 - learning_rate: 7.5000e-05\nEpoch 9/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - auc_roc: 0.9410 - f1: 0.8130 - loss: 0.2988 - precision: 0.8274 - recall: 0.7992\nEpoch 9: val_auc_roc improved from 0.90757 to 0.90906, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 197ms/step - auc_roc: 0.9410 - f1: 0.8129 - loss: 0.2989 - precision: 0.8274 - recall: 0.7992 - val_auc_roc: 0.9091 - val_f1: 0.6387 - val_loss: 0.2472 - val_precision: 0.6333 - val_recall: 0.6441 - learning_rate: 7.5000e-05\nEpoch 10/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9426 - f1: 0.8169 - loss: 0.2937 - precision: 0.8211 - recall: 0.8128\nEpoch 10: val_auc_roc improved from 0.90906 to 0.91054, saving model to vit_image.keras\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 196ms/step - auc_roc: 0.9426 - f1: 0.8169 - loss: 0.2937 - precision: 0.8211 - recall: 0.8128 - val_auc_roc: 0.9105 - val_f1: 0.6541 - val_loss: 0.2794 - val_precision: 0.5878 - val_recall: 0.7373 - learning_rate: 7.5000e-05\nEpoch 11/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9440 - f1: 0.8366 - loss: 0.2914 - precision: 0.8346 - recall: 0.8388\nEpoch 11: val_auc_roc did not improve from 0.91054\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 181ms/step - auc_roc: 0.9440 - f1: 0.8365 - loss: 0.2914 - precision: 0.8346 - recall: 0.8387 - val_auc_roc: 0.9090 - val_f1: 0.6303 - val_loss: 0.2465 - val_precision: 0.6250 - val_recall: 0.6356 - learning_rate: 7.5000e-05\nEpoch 12/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9435 - f1: 0.8203 - loss: 0.2924 - precision: 0.8139 - recall: 0.8278\nEpoch 12: val_auc_roc did not improve from 0.91054\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 181ms/step - auc_roc: 0.9435 - f1: 0.8202 - loss: 0.2924 - precision: 0.8140 - recall: 0.8277 - val_auc_roc: 0.9096 - val_f1: 0.6420 - val_loss: 0.2486 - val_precision: 0.6240 - val_recall: 0.6610 - learning_rate: 7.5000e-05\nEpoch 13/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9462 - f1: 0.8209 - loss: 0.2873 - precision: 0.8283 - recall: 0.8138\nEpoch 13: val_auc_roc did not improve from 0.91054\n\nEpoch 13: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 180ms/step - auc_roc: 0.9462 - f1: 0.8209 - loss: 0.2872 - precision: 0.8284 - recall: 0.8138 - val_auc_roc: 0.9091 - val_f1: 0.6641 - val_loss: 0.2781 - val_precision: 0.6042 - val_recall: 0.7373 - learning_rate: 7.5000e-05\nEpoch 14/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9497 - f1: 0.8297 - loss: 0.2779 - precision: 0.8451 - recall: 0.8154\nEpoch 14: val_auc_roc did not improve from 0.91054\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 180ms/step - auc_roc: 0.9497 - f1: 0.8298 - loss: 0.2778 - precision: 0.8451 - recall: 0.8155 - val_auc_roc: 0.9091 - val_f1: 0.6484 - val_loss: 0.2609 - val_precision: 0.6014 - val_recall: 0.7034 - learning_rate: 3.7500e-05\nEpoch 15/20\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - auc_roc: 0.9507 - f1: 0.8419 - loss: 0.2722 - precision: 0.8470 - recall: 0.8374\nEpoch 15: val_auc_roc did not improve from 0.91054\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 1.8750000890577212e-05.\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 180ms/step - auc_roc: 0.9507 - f1: 0.8419 - loss: 0.2721 - precision: 0.8470 - recall: 0.8374 - val_auc_roc: 0.9089 - val_f1: 0.6434 - val_loss: 0.2653 - val_precision: 0.5929 - val_recall: 0.7034 - learning_rate: 3.7500e-05\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"final = tf.keras.models.load_model(\n    \"/kaggle/input/intras-1/best_vit_tab.keras\",\n    custom_objects={\n        \"PatchExtractor\": PatchExtractor,\n        \"AddClassTokenAndPos\": AddClassTokenAndPos,\n        \"transformer_encoder\": transformer_encoder\n    },\n    compile=False  # Skip loading loss/metrics state\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:26:37.993637Z","iopub.execute_input":"2025-08-16T04:26:37.993977Z","iopub.status.idle":"2025-08-16T04:26:42.386058Z","shell.execute_reply.started":"2025-08-16T04:26:37.993949Z","shell.execute_reply":"2025-08-16T04:26:42.385005Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'patch_extractor_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"X_train_img_tf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:50:49.054338Z","iopub.execute_input":"2025-08-16T05:50:49.054919Z","iopub.status.idle":"2025-08-16T05:50:49.059871Z","shell.execute_reply.started":"2025-08-16T05:50:49.054895Z","shell.execute_reply":"2025-08-16T05:50:49.059070Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"TensorShape([4855, 224, 224, 3])"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred = final.predict([X_test_img_tf,  X_test_tab_scaled])\nfor threshold in np.linspace(0, 0.5, 20):\n    y_ = y_pred > threshold\n    print(f'threshold = {threshold}')\n    print(classification_report(y_test_, y_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:50:54.915248Z","iopub.execute_input":"2025-08-16T05:50:54.915519Z","iopub.status.idle":"2025-08-16T05:50:54.934133Z","shell.execute_reply.started":"2025-08-16T05:50:54.915498Z","shell.execute_reply":"2025-08-16T05:50:54.933130Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1677664431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_img_tf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX_test_tab_scaled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'threshold = {threshold}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'final' is not defined"],"ename":"NameError","evalue":"name 'final' is not defined","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"class F1ScoreWithThreshold(tf.keras.metrics.Metric):\n    def __init__(self, threshold=0.3, name=\"f1_score_with_threshold\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.threshold = threshold\n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        p = self.precision.result()\n        r = self.recall.result()\n        return 2 * p * r / (p + r + 1e-8)  # F1 formula with epsilon to prevent divide by zero\n\n    def reset_states(self):\n        self.precision.reset_states()\n        self.recall.reset_states()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:40.044702Z","iopub.execute_input":"2025-08-16T06:33:40.045351Z","iopub.status.idle":"2025-08-16T06:33:40.051151Z","shell.execute_reply.started":"2025-08-16T06:33:40.045316Z","shell.execute_reply":"2025-08-16T06:33:40.050473Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.Resize(128),\n    transforms.ToTensor()\n])\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, pil_images, targets, X, transform=None):\n        self.X = X\n        self.images = pil_images\n        self.targets = targets\n        self.transform = transform\n        assert len(self.images) == len(self.X) == len(self.targets), \"Length mismatch\"\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n        label = self.targets[idx]\n        return image, self.X.iloc[idx].values, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:42.168185Z","iopub.execute_input":"2025-08-16T06:33:42.168928Z","iopub.status.idle":"2025-08-16T06:33:42.174415Z","shell.execute_reply.started":"2025-08-16T06:33:42.168905Z","shell.execute_reply":"2025-08-16T06:33:42.173587Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"training = CustomDataset(images_train_new, y_train_new, X_train_new.drop(columns=['images_idx']), preprocess)\ntesting = CustomDataset(images_test, y_test, X_test.drop(columns=['images_idx']), preprocess)\ntrain_dataloader = DataLoader(training, batch_size=128, shuffle = True, pin_memory = True)\ntest_dataloader = DataLoader(testing, batch_size=128, shuffle = True, pin_memory = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:22:10.762529Z","iopub.execute_input":"2025-08-16T06:22:10.763216Z","iopub.status.idle":"2025-08-16T06:22:10.769799Z","shell.execute_reply.started":"2025-08-16T06:22:10.763191Z","shell.execute_reply":"2025-08-16T06:22:10.769242Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"def dataloader_to_numpy(dataloader):\n    images, X, y = [], [], []\n    for imgs, x, labels in dataloader:\n        imgs = imgs.permute(0, 2, 3, 1)  # (B, H, W, C)\n        images.append(imgs)\n        X.append(x)\n        y.append(labels)\n    return np.concatenate(images), np.concatenate(y), np.concatenate(X)\n\nX_train_img, y_train_, X_train_tab_ = dataloader_to_numpy(train_dataloader)\nX_test_img, y_test_, X_test_tab_ = dataloader_to_numpy(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:22:14.820610Z","iopub.execute_input":"2025-08-16T06:22:14.820845Z","iopub.status.idle":"2025-08-16T06:22:20.548269Z","shell.execute_reply.started":"2025-08-16T06:22:14.820829Z","shell.execute_reply":"2025-08-16T06:22:20.547655Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"class F1ScoreWithThreshold(tf.keras.metrics.Metric):\n    def __init__(self, threshold=0.3, name=\"f1_score_with_threshold\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.threshold = threshold\n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        p = self.precision.result()\n        r = self.recall.result()\n        return 2 * p * r / (p + r + 1e-8)  # F1 formula with epsilon to prevent divide by zero\n\n    def reset_states(self):\n        self.precision.reset_states()\n        self.recall.reset_states()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:33:46.003697Z","iopub.execute_input":"2025-08-16T06:33:46.004387Z","iopub.status.idle":"2025-08-16T06:33:46.009670Z","shell.execute_reply.started":"2025-08-16T06:33:46.004348Z","shell.execute_reply":"2025-08-16T06:33:46.009003Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50, EfficientNetV2M\nfrom tensorflow.keras.layers import (\n    Input, Dense, Concatenate, BatchNormalization, Dropout, GlobalAveragePooling2D, MaxPooling2D, Flatten\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import AdamW, Adam\nfrom tensorflow.keras.initializers import HeNormal\nimport tensorflow as tf\n\n# Custom Focal Loss and Custom Metric\ndef binary_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n        weight = alpha * tf.pow(1 - y_pred, gamma) * y_true + (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true)\n        return tf.reduce_mean(weight * cross_entropy)\n    return loss\n\nclass RecallWithThreshold(tf.keras.metrics.Metric):\n    def __init__(self, threshold=0.3, name=\"recall_with_threshold\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.threshold = threshold\n        self.recall = tf.keras.metrics.Recall()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        return self.recall.result()\n\n    def reset_states(self):\n        self.recall.reset_states()\n\n# -------- Inputs -------- \nimage_input = Input(shape=(128, 128, 3), name='image_input')\nnum_input = Input(shape=(34,), name='num_input')\n\n# -------- Image branch with ResNet50 --------\nbase_model = ResNet50(weights='imagenet', include_top=False, input_tensor=image_input)\nbase_model.trainable = False  # freeze ResNet weights\n\nx1 = base_model.output\nx1 = MaxPooling2D((2, 2))(x1)\nx1 = Flatten()(x1)\n\nx1 = Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x1)\nx1 = BatchNormalization()(x1)\nx1 = Dropout(0.2)(x1)\n\nx1 = Dense(32, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x1)\nx1 = BatchNormalization()(x1)\nx1 = Dropout(0.2)(x1)\n\nx1 = Dense(16, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x1)\n\n# -------- Numerical branch --------\nx2 = BatchNormalization()(num_input)\nx2 = Dense(48, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x2)\nx2 = BatchNormalization()(x2)\nx2 = Dropout(0.2)(x2)\n\nx2 = Dense(32, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x2)\nx2 = BatchNormalization()(x2)\nx2 = Dropout(0.2)(x2)\n\nx2 = Dense(16, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(x2)\n\n# -------- Merge branches --------\nmerged = Concatenate()([x1, x2])\n\nmerged = BatchNormalization()(merged)\nmerged = Dropout(0.2)(merged)\nmerged = Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dropout(0.2)(merged)\nmerged = Dense(16, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=regularizers.l2(1e-4))(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dropout(0.2)(merged)\noutput = Dense(1, activation='sigmoid')(merged)\n\n# -------- Build and Compile Model --------\nmodel = Model(inputs=[image_input, num_input], outputs=output)\n\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-3),\n    loss=binary_focal_loss(),\n    metrics=['precision', F1ScoreWithThreshold(), RecallWithThreshold()]\n)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:34:47.511300Z","iopub.execute_input":"2025-08-16T06:34:47.512062Z","iopub.status.idle":"2025-08-16T06:34:48.876587Z","shell.execute_reply.started":"2025-08-16T06:34:47.512036Z","shell.execute_reply":"2025-08-16T06:34:48.875891Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_5 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_pad           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m134\u001b[0m, \u001b[38;5;34m134\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ cast_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_conv (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │      \u001b[38;5;34m9,472\u001b[0m │ conv1_pad[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_bn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv1_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_relu          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pool1_pad           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m66\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv1_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pool1_pool          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ pool1_pad[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m4,160\u001b[0m │ pool1_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block1_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block1_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ conv2_block1_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block1_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block1_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_0_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,640\u001b[0m │ pool1_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,640\u001b[0m │ conv2_block1_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_0_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ conv2_block1_0_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ conv2_block1_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block1_0_b… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block1_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,448\u001b[0m │ conv2_block1_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block2_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block2_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ conv2_block2_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block2_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block2_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,640\u001b[0m │ conv2_block2_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ conv2_block2_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block1_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block2_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,448\u001b[0m │ conv2_block2_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block3_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block3_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ conv2_block3_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2_block3_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block3_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m16,640\u001b[0m │ conv2_block3_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m1,024\u001b[0m │ conv2_block3_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block2_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m256\u001b[0m)              │            │ conv2_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2_block3_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m32,896\u001b[0m │ conv2_block3_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block1_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block1_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv3_block1_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block1_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block1_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_0_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │    \u001b[38;5;34m131,584\u001b[0m │ conv2_block3_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m66,048\u001b[0m │ conv3_block1_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_0_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv3_block1_0_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv3_block1_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block1_0_b… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m512\u001b[0m)              │            │ conv3_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block1_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m65,664\u001b[0m │ conv3_block1_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block2_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block2_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv3_block2_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block2_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block2_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m66,048\u001b[0m │ conv3_block2_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv3_block2_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block1_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m512\u001b[0m)              │            │ conv3_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block2_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m65,664\u001b[0m │ conv3_block2_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block3_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block3_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv3_block3_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block3_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block3_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m66,048\u001b[0m │ conv3_block3_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv3_block3_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block2_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m512\u001b[0m)              │            │ conv3_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block3_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m65,664\u001b[0m │ conv3_block3_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block4_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block4_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv3_block4_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv3_block4_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block4_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │     \u001b[38;5;34m66,048\u001b[0m │ conv3_block4_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv3_block4_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block3_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m512\u001b[0m)              │            │ conv3_block4_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv3_block4_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m131,328\u001b[0m │ conv3_block4_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block1_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block1_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block1_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block1_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block1_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_0_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m525,312\u001b[0m │ conv3_block4_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block1_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_0_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block1_0_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block1_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block1_0_b… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block1_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m262,400\u001b[0m │ conv4_block1_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block2_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block2_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block2_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block2_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block2_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block2_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block2_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block1_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block2_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m262,400\u001b[0m │ conv4_block2_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block3_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block3_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block3_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block3_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block3_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block3_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block3_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block2_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block3_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m262,400\u001b[0m │ conv4_block3_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block4_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block4_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block4_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block4_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block4_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block4_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block4_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block3_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block4_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block4_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m262,400\u001b[0m │ conv4_block4_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block5_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block5_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block5_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block5_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block5_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block5_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block5_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block4_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block5_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block5_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m262,400\u001b[0m │ conv4_block5_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block6_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block6_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m590,080\u001b[0m │ conv4_block6_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │      \u001b[38;5;34m1,024\u001b[0m │ conv4_block6_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv4_block6_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │    \u001b[38;5;34m263,168\u001b[0m │ conv4_block6_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │      \u001b[38;5;34m4,096\u001b[0m │ conv4_block6_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block5_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m1024\u001b[0m)             │            │ conv4_block6_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv4_block6_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m1024\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m524,800\u001b[0m │ conv4_block6_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block1_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block1_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m2,359,808\u001b[0m │ conv5_block1_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block1_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block1_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_0_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │  \u001b[38;5;34m2,099,200\u001b[0m │ conv4_block6_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │  \u001b[38;5;34m1,050,624\u001b[0m │ conv5_block1_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_0_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │      \u001b[38;5;34m8,192\u001b[0m │ conv5_block1_0_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │      \u001b[38;5;34m8,192\u001b[0m │ conv5_block1_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block1_0_b… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m2048\u001b[0m)             │            │ conv5_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block1_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,049,088\u001b[0m │ conv5_block1_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block2_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block2_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m2,359,808\u001b[0m │ conv5_block2_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block2_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block2_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │  \u001b[38;5;34m1,050,624\u001b[0m │ conv5_block2_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │      \u001b[38;5;34m8,192\u001b[0m │ conv5_block2_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block1_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m2048\u001b[0m)             │            │ conv5_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block2_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,049,088\u001b[0m │ conv5_block2_out… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block3_1_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block3_1_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m2,359,808\u001b[0m │ conv5_block3_1_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │      \u001b[38;5;34m2,048\u001b[0m │ conv5_block3_2_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_relu │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv5_block3_2_b… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_3_conv │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │  \u001b[38;5;34m1,050,624\u001b[0m │ conv5_block3_2_r… │\n│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_3_bn   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │      \u001b[38;5;34m8,192\u001b[0m │ conv5_block3_3_c… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_add    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block2_out… │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │ \u001b[38;5;34m2048\u001b[0m)             │            │ conv5_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_out    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block3_add… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ num_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv5_block3_out… │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_6 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ num_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │        \u001b[38;5;34m136\u001b[0m │ cast_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │    \u001b[38;5;34m524,352\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        │      \u001b[38;5;34m1,680\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        │        \u001b[38;5;34m192\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_44          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_32 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m1,568\u001b[0m │ dropout_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_45          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dropout_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_33 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dropout_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_46          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_34 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,112\u001b[0m │ dropout_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_35 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m1,040\u001b[0m │ dropout_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m64\u001b[0m │ dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_36 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_pad           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">134</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">134</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cast_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │ conv1_pad[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_bn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1_relu          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pool1_pad           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ pool1_pool          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool1_pad[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ pool1_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block1_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2_block1_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block1_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_0_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ pool1_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ conv2_block1_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_0_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block1_0_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block1_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_0_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block1_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ conv2_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block2_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2_block2_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block2_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ conv2_block2_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block2_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block2_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ conv2_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block3_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2_block3_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2_block3_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ conv2_block3_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block3_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │ conv2_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2_block3_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ conv2_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block1_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv3_block1_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block1_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_0_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ conv2_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ conv3_block1_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_0_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block1_0_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block1_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_0_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv3_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block1_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ conv3_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block2_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv3_block2_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block2_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ conv3_block2_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block2_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv3_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block2_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ conv3_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block3_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv3_block3_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block3_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ conv3_block3_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block3_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv3_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block3_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ conv3_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block4_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv3_block4_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block4_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ conv3_block4_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block4_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │ conv3_block4_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3_block4_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ conv3_block4_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block1_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block1_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block1_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_0_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ conv3_block4_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block1_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_0_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block1_0_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block1_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_0_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block1_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ conv4_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block2_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block2_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block2_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block2_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block2_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block2_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ conv4_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block3_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block3_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block3_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block3_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block3_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block3_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ conv4_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block4_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block4_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block4_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block4_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block4_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block4_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block4_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ conv4_block4_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block5_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block5_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block5_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block5_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block5_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block5_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block5_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │ conv4_block5_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block6_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ conv4_block6_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv4_block6_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ conv4_block6_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block6_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ conv4_block6_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv4_block6_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ conv4_block6_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block1_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ conv5_block1_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block1_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_0_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │ conv4_block6_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ conv5_block1_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_0_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv5_block1_0_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv5_block1_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_0_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │ conv5_block1_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block1_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ conv5_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block2_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ conv5_block2_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block2_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ conv5_block2_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv5_block2_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │ conv5_block2_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block2_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ conv5_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block3_1_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_1_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_1_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ conv5_block3_1_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv5_block3_2_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_2_relu │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_2_b… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_3_conv │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ conv5_block3_2_r… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_3_bn   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv5_block3_3_c… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_add    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │ conv5_block3_3_b… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv5_block3_out    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_add… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ num_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_out… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ num_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ cast_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,352</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,680</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_44          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │ dropout_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_45          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dropout_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dropout_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_46          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ dropout_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │ dropout_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,122,905\u001b[0m (92.02 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,122,905</span> (92.02 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m534,549\u001b[0m (2.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">534,549</span> (2.04 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,588,356\u001b[0m (89.98 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,588,356</span> (89.98 MB)\n</pre>\n"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"# LOADING MODELS ","metadata":{}},{"cell_type":"code","source":"vit = tf.keras.models.load_model(\n    \"/kaggle/input/intras-1/best_vit_tab.keras\",\n    custom_objects={\n        \"PatchExtractor\": PatchExtractor,\n        \"AddClassTokenAndPos\": AddClassTokenAndPos,\n        \"transformer_encoder\": transformer_encoder\n    },\n    compile=False  # Skip loading loss/metrics state\n)\nresnet = tf.keras.models.load_model(\n    \"/kaggle/input/intras-1/RESNET_MODEL.h5\",\n    custom_objects={\n        \"binary_focal_loss\" : binary_focal_loss()\n    },\n    compile=False  # Skip loading loss/metrics state\n)\neffnet = tf.keras.models.load_model(\n    \"/kaggle/input/intras-1/EFFICIENT_NET.h5\",\n    custom_objects={\n        \"binary_focal_loss\" : binary_focal_loss()\n    },\n    compile=False  # Skip loading loss/metrics state\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:35:38.501250Z","iopub.execute_input":"2025-08-16T06:35:38.501878Z","iopub.status.idle":"2025-08-16T06:35:47.767067Z","shell.execute_reply.started":"2025-08-16T06:35:38.501855Z","shell.execute_reply":"2025-08-16T06:35:47.766490Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"vit_im_ex = tf.keras.models.load_model(\n    \"/kaggle/working/vit_image.keras\",\n    custom_objects={\n        \"PatchExtractor\": PatchExtractor,\n        \"AddClassTokenAndPos\": AddClassTokenAndPos,\n        \"transformer_encoder\": transformer_encoder\n    },\n    compile=False  # Skip loading loss/metrics state\n)\nvit_im_ol = tf.keras.models.Model(\n    inputs=vit_im_ex.input,\n    outputs=vit_im_ex.get_layer(\"image_features\").output\n)\nX_train_vit_im = vit_im_ol.predict([X_train_img_tf, X_train_tab_scaled])\nX_test_vit_im = vit_im_ol.predict([X_test_img_tf, X_test_tab_scaled])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:36:36.964413Z","iopub.execute_input":"2025-08-16T06:36:36.964673Z","iopub.status.idle":"2025-08-16T06:37:00.171543Z","shell.execute_reply.started":"2025-08-16T06:36:36.964654Z","shell.execute_reply":"2025-08-16T06:37:00.170874Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'patch_extractor_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: {'image': 'image', 'tab': 'tab'}\nReceived: inputs=('Tensor(shape=(32, 224, 224, 3))', 'Tensor(shape=(32, 34))')\n  warnings.warn(msg)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1755326203.538451     666 service.cc:148] XLA service 0x789da8008cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1755326203.539040     666 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1755326203.539065     666 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1755326203.942284     666 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  3/152\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755326206.824357     666 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: {'image': 'image', 'tab': 'tab'}\nReceived: inputs=('Tensor(shape=(None, 224, 224, 3))', 'Tensor(shape=(None, 34))')\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 66ms/step\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# vit output\nvit_ol = tf.keras.models.Model(\n    inputs=vit.input,\n    outputs=vit.get_layer(\"batch_normalization_21\").output\n)\nX_train_vit = vit_ol.predict([X_train_img_tf, X_train_tab_scaled])\nX_test_vit = vit_ol.predict([X_test_img_tf, X_test_tab_scaled])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:21:18.541015Z","iopub.execute_input":"2025-08-16T06:21:18.541612Z","iopub.status.idle":"2025-08-16T06:21:34.074018Z","shell.execute_reply.started":"2025-08-16T06:21:18.541590Z","shell.execute_reply":"2025-08-16T06:21:34.073448Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# resnet output \nresnet_ol = tf.keras.models.Model(\n    inputs=resnet.input,\n    outputs=resnet.get_layer(\"dense_11\").output\n)\nX_train_resnet = resnet_ol.predict([X_train_img, X_train_tab_])\nX_test_resnet = resnet_ol.predict([X_test_img, X_test_tab_])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:47:21.697743Z","iopub.execute_input":"2025-08-16T06:47:21.698065Z","iopub.status.idle":"2025-08-16T06:47:41.297727Z","shell.execute_reply.started":"2025-08-16T06:47:21.698042Z","shell.execute_reply":"2025-08-16T06:47:41.296983Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# effnet output\neffnet_ol = tf.keras.models.Model(\n    inputs=effnet.input,\n    outputs=effnet.get_layer(\"dense_20\").output\n)\nX_train_effnet = effnet_ol.predict([X_train_img, X_train_tab_])\nX_test_effnet = effnet_ol.predict([X_test_img, X_test_tab_])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:48:12.202312Z","iopub.execute_input":"2025-08-16T06:48:12.203060Z","iopub.status.idle":"2025-08-16T06:48:57.051302Z","shell.execute_reply.started":"2025-08-16T06:48:12.203033Z","shell.execute_reply":"2025-08-16T06:48:57.050698Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 146ms/step\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 231ms/step\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"!pip install autogluon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:55:09.085263Z","iopub.execute_input":"2025-08-16T06:55:09.086235Z","iopub.status.idle":"2025-08-16T07:02:55.361814Z","shell.execute_reply.started":"2025-08-16T06:55:09.086202Z","shell.execute_reply":"2025-08-16T07:02:55.360993Z"}},"outputs":[{"name":"stdout","text":"Collecting autogluon\n  Downloading autogluon-1.4.0-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.core==1.4.0 (from autogluon.core[all]==1.4.0->autogluon)\n  Downloading autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.features==1.4.0 (from autogluon)\n  Downloading autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.tabular==1.4.0 (from autogluon.tabular[all]==1.4.0->autogluon)\n  Downloading autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\nCollecting autogluon.multimodal==1.4.0 (from autogluon)\n  Downloading autogluon.multimodal-1.4.0-py3-none-any.whl.metadata (13 kB)\nCollecting autogluon.timeseries==1.4.0 (from autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading autogluon.timeseries-1.4.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.26.4)\nRequirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.15.3)\nCollecting scikit-learn<1.8.0,>=1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.5)\nRequirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.2.3)\nRequirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.32.4)\nRequirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.7.2)\nRequirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.39.1)\nCollecting autogluon.common==1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Downloading autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.4.0->autogluon) (19.0.1)\nCollecting ray<2.45,>=2.10.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: Pillow<12,>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (11.2.1)\nRequirement already satisfied: torch<2.8,>=2.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.6.0+cu124)\nCollecting lightning<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\nCollecting transformers<4.50,>=4.38.0 (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon)\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate<2.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.8.1)\nCollecting fsspec<=2025.3 (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting jsonschema<4.24,>=4.18 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nCollecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision<0.23.0,>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.21.0+cu124)\nRequirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\nRequirement already satisfied: torchmetrics<1.8,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.7.3)\nRequirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\nCollecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: nltk<3.10,>=3.4.5 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.1)\nCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.6)\nRequirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.18.0)\nRequirement already satisfied: pytesseract<0.4,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.13)\nCollecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: catboost<1.3,>=1.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.2.8)\nRequirement already satisfied: fastai<2.9,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.7.19)\nCollecting loguru (from autogluon.tabular[all]==1.4.0->autogluon)\n  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: lightgbm<4.7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.5.0)\nCollecting einx (from autogluon.tabular[all]==1.4.0->autogluon)\n  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: xgboost<3.1,>=2.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.0.3)\nRequirement already satisfied: spacy<3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.7)\nRequirement already satisfied: huggingface-hub[torch] in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.33.1)\nRequirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.5.1)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.5.2)\nCollecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading gluonts-0.16.2-py3-none-any.whl.metadata (9.8 kB)\nCollecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\nCollecting mlforecast<0.15.0,>=0.14.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading mlforecast-0.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting utilsforecast<0.2.12,>=0.2.3 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\nCollecting coreforecast<0.0.17,>=0.0.12 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading coreforecast-0.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting fugue>=0.9.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.10.18)\nRequirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (7.0.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (25.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (0.5.3)\nRequirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.39.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.13.0)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\nRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (24.1.2)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.7.29)\nRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.12.13)\nCollecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\nCollecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.11.7)\nCollecting toolz~=0.10 (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.14.0)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (3.1.1)\nRequirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.25.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.14.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.9.0.post0)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.60.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.4.0)\nCollecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (8.2.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (2024.11.6)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.4.1)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.4.6)\nCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (14.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.18.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.1.1)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.20.3)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.7.0)\nCollecting aiohttp_cors (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\nCollecting colorful (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading colorful-0.5.7-py2.py3-none-any.whl.metadata (16 kB)\nCollecting py-spy>=0.2.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\nRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.73.1)\nCollecting opencensus (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.22.1)\nRequirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (7.1.0)\nCollecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting tensorboardX>=1.9 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.6.15)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.16.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.4)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.1.3)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.21.2)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.0)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.6)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[torch]; extra == \"all\"->autogluon.tabular[all]==1.4.0->autogluon) (1.1.5)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2.6.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.20.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.13.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.43.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\nCollecting numpy<2.4.0,>=1.25.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.19.2)\nCollecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.3.8)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.21.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.17.2)\nCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\nCollecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.34.1)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.23.0)\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.16.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.0.41)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (8.5.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.3.10)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.70.0)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.40.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.2.3)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.7)\nCollecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting filelock (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting packaging>=20.0 (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pytz>=2020.1 (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\nINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\nINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (1.7.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.9.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.6.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\n  Downloading mkl_fft-1.3.14-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Downloading mkl_fft-1.3.13-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Downloading mkl_fft-1.3.11-81-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nCollecting patsy>=0.5.6 (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\nINFO: pip is still looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\n  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon)\n  Downloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nCollecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Downloading window_ops-0.0.14-py3-none-any.whl.metadata (6.9 kB)\n  Downloading window_ops-0.0.13-py3-none-any.whl.metadata (5.0 kB)\n  Downloading window_ops-0.0.12-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.11-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.10-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.9-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.8-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.7-py3-none-any.whl.metadata (4.9 kB)\n  Downloading window_ops-0.0.6-py3-none-any.whl.metadata (3.4 kB)\n  Downloading window_ops-0.0.5-py3-none-any.whl.metadata (4.6 kB)\n  Downloading window_ops-0.0.4-py3-none-any.whl.metadata (4.7 kB)\n  Downloading window_ops-0.0.3-py3-none-any.whl.metadata (3.5 kB)\n  Downloading window_ops-0.0.2-py3-none-any.whl.metadata (3.5 kB)\n  Downloading window_ops-0.0.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting safetensors[torch] (from huggingface-hub[torch]; extra == \"all\"->autogluon.tabular[all]==1.4.0->autogluon)\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n  Downloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.5.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.5.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"!pip install autogluon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:03:10.354408Z","iopub.execute_input":"2025-08-16T07:03:10.354747Z","iopub.status.idle":"2025-08-16T07:07:48.226515Z","shell.execute_reply.started":"2025-08-16T07:03:10.354725Z","shell.execute_reply":"2025-08-16T07:07:48.225642Z"}},"outputs":[{"name":"stdout","text":"Collecting autogluon\n  Using cached autogluon-1.4.0-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.core==1.4.0 (from autogluon.core[all]==1.4.0->autogluon)\n  Using cached autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.features==1.4.0 (from autogluon)\n  Using cached autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.tabular==1.4.0 (from autogluon.tabular[all]==1.4.0->autogluon)\n  Using cached autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\nCollecting autogluon.multimodal==1.4.0 (from autogluon)\n  Using cached autogluon.multimodal-1.4.0-py3-none-any.whl.metadata (13 kB)\nCollecting autogluon.timeseries==1.4.0 (from autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached autogluon.timeseries-1.4.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.26.4)\nRequirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.15.3)\nCollecting scikit-learn<1.8.0,>=1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Using cached scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.5)\nRequirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.2.3)\nRequirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.32.4)\nRequirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.7.2)\nRequirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.39.1)\nCollecting autogluon.common==1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Using cached autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.4.0->autogluon) (19.0.1)\nCollecting ray<2.45,>=2.10.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: Pillow<12,>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (11.2.1)\nRequirement already satisfied: torch<2.8,>=2.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.6.0+cu124)\nCollecting lightning<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\nCollecting transformers<4.50,>=4.38.0 (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon)\n  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: accelerate<2.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.8.1)\nCollecting fsspec<=2025.3 (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon)\n  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting jsonschema<4.24,>=4.18 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached seqeval-1.2.2.tar.gz (43 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nCollecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached timm-1.0.3-py3-none-any.whl.metadata (43 kB)\nRequirement already satisfied: torchvision<0.23.0,>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.21.0+cu124)\nRequirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\nRequirement already satisfied: torchmetrics<1.8,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.7.3)\nRequirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\nCollecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: nltk<3.10,>=3.4.5 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.1)\nCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.6)\nRequirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.18.0)\nRequirement already satisfied: pytesseract<0.4,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.13)\nCollecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: catboost<1.3,>=1.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.2.8)\nRequirement already satisfied: fastai<2.9,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.7.19)\nCollecting loguru (from autogluon.tabular[all]==1.4.0->autogluon)\n  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: lightgbm<4.7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.5.0)\nCollecting einx (from autogluon.tabular[all]==1.4.0->autogluon)\n  Using cached einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: xgboost<3.1,>=2.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.0.3)\nRequirement already satisfied: spacy<3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.7)\nRequirement already satisfied: huggingface-hub[torch] in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.33.1)\nRequirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.5.1)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.5.2)\nCollecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached gluonts-0.16.2-py3-none-any.whl.metadata (9.8 kB)\nCollecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\nCollecting mlforecast<0.15.0,>=0.14.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached mlforecast-0.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting utilsforecast<0.2.12,>=0.2.3 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\nCollecting coreforecast<0.0.17,>=0.0.12 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached coreforecast-0.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting fugue>=0.9.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.10.18)\nRequirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (7.0.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (25.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (0.5.3)\nRequirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.39.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.13.0)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\nRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (24.1.2)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.7.29)\nRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.12.13)\nCollecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\nCollecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.11.7)\nCollecting toolz~=0.10 (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.14.0)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (3.1.1)\nRequirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.25.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.14.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.9.0.post0)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.60.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.4.0)\nCollecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (8.2.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (2024.11.6)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.4.1)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.4.6)\nCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (14.0.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.18.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.1.1)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.20.3)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.7.0)\nCollecting aiohttp_cors (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\nCollecting colorful (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached colorful-0.5.7-py2.py3-none-any.whl.metadata (16 kB)\nCollecting py-spy>=0.2.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\nRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.73.1)\nCollecting opencensus (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.22.1)\nRequirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (7.1.0)\nCollecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting tensorboardX>=1.9 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.6.15)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.16.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.4)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.1.3)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.21.2)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.0)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.6)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[torch]; extra == \"all\"->autogluon.tabular[all]==1.4.0->autogluon) (1.1.5)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2.6.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.20.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.13.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.43.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\nCollecting numpy<2.4.0,>=1.25.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Using cached numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.19.2)\nCollecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.3.8)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.21.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.17.2)\nCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\nCollecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.34.1)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.23.0)\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.16.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.0.41)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (8.5.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.3.10)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.70.0)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.40.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.2.3)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.7)\nCollecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting filelock (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached oss2-2.17.0.tar.gz (259 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting packaging>=20.0 (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon)\n  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pytz>=2020.1 (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Using cached pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\nINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n  Using cached openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\nINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n  Using cached openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n  Using cached openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n  Using cached openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n  Using cached openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n  Using cached openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (1.7.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.9.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.6.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy<2.4.0,>=1.25.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n  Using cached mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\n  Using cached mkl_fft-1.3.14-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Using cached mkl_fft-1.3.13-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n  Using cached mkl_fft-1.3.11-81-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nCollecting patsy>=0.5.6 (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\nINFO: pip is still looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\n  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon)\n  Using cached blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nCollecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n  Using cached window_ops-0.0.14-py3-none-any.whl.metadata (6.9 kB)\n  Using cached window_ops-0.0.13-py3-none-any.whl.metadata (5.0 kB)\n  Using cached window_ops-0.0.12-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.11-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.10-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.9-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.8-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.7-py3-none-any.whl.metadata (4.9 kB)\n  Using cached window_ops-0.0.6-py3-none-any.whl.metadata (3.4 kB)\n  Using cached window_ops-0.0.5-py3-none-any.whl.metadata (4.6 kB)\n  Using cached window_ops-0.0.4-py3-none-any.whl.metadata (4.7 kB)\n  Using cached window_ops-0.0.3-py3-none-any.whl.metadata (3.5 kB)\n  Using cached window_ops-0.0.2-py3-none-any.whl.metadata (3.5 kB)\n  Using cached window_ops-0.0.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting safetensors[torch] (from huggingface-hub[torch]; extra == \"all\"->autogluon.tabular[all]==1.4.0->autogluon)\n  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n  Using cached safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.5.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.5.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import sys\nprint(sys.version)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:07:56.643303Z","iopub.execute_input":"2025-08-16T07:07:56.643910Z","iopub.status.idle":"2025-08-16T07:07:56.649057Z","shell.execute_reply.started":"2025-08-16T07:07:56.643883Z","shell.execute_reply":"2025-08-16T07:07:56.648214Z"}},"outputs":[{"name":"stdout","text":"3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"!pip install flaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:09:28.706195Z","iopub.execute_input":"2025-08-16T07:09:28.706802Z","iopub.status.idle":"2025-08-16T07:09:32.473970Z","shell.execute_reply.started":"2025-08-16T07:09:28.706775Z","shell.execute_reply":"2025-08-16T07:09:32.473208Z"}},"outputs":[{"name":"stdout","text":"Collecting flaml\n  Downloading FLAML-2.3.6-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: NumPy>=1.17 in /usr/local/lib/python3.11/dist-packages (from flaml) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from NumPy>=1.17->flaml) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->NumPy>=1.17->flaml) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->NumPy>=1.17->flaml) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->NumPy>=1.17->flaml) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->NumPy>=1.17->flaml) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->NumPy>=1.17->flaml) (2024.2.0)\nDownloading FLAML-2.3.6-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: flaml\nSuccessfully installed flaml-2.3.6\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train_new.shape)\nprint(len(y_train_new))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:10:44.154175Z","iopub.execute_input":"2025-08-16T07:10:44.154477Z","iopub.status.idle":"2025-08-16T07:10:44.159439Z","shell.execute_reply.started":"2025-08-16T07:10:44.154453Z","shell.execute_reply":"2025-08-16T07:10:44.158428Z"}},"outputs":[{"name":"stdout","text":"(4855, 35)\n4855\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"from flaml import AutoML\n\nautoml = AutoML()\n\nautoml_settings = {\n    \"time_budget\": 60,\n    \"task\": \"classification\",\n    \"metric\": \"accuracy\",\n    \"log_file_name\": \"flaml.log\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:12:51.217189Z","iopub.execute_input":"2025-08-16T07:12:51.217777Z","iopub.status.idle":"2025-08-16T07:13:01.805271Z","shell.execute_reply.started":"2025-08-16T07:12:51.217749Z","shell.execute_reply":"2025-08-16T07:13:01.804579Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"automl.fit(X_train=X_train_new, y_train = y_train_, **automl_settings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:13:35.257660Z","iopub.execute_input":"2025-08-16T07:13:35.259196Z","iopub.status.idle":"2025-08-16T07:14:35.308517Z","shell.execute_reply.started":"2025-08-16T07:13:35.259168Z","shell.execute_reply":"2025-08-16T07:14:35.307860Z"}},"outputs":[{"name":"stdout","text":"[flaml.automl.logger: 08-16 07:13:35] {1752} INFO - task = classification\n[flaml.automl.logger: 08-16 07:13:35] {1763} INFO - Evaluation method: holdout\n[flaml.automl.logger: 08-16 07:13:35] {1862} INFO - Minimizing error metric: 1-accuracy\n[flaml.automl.logger: 08-16 07:13:35] {1979} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 0, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2417} INFO - Estimated sufficient time budget=544s. Estimated necessary time budget=13s.\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.3s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 1, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 2, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 3, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 4, current learner sgd\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.4s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 5, current learner sgd\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.5s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 6, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 7, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 8, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 9, current learner sgd\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.6s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 10, current learner sgd\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.6s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 11, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.7s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 12, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:35] {2466} INFO -  at 0.7s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:35] {2282} INFO - iteration 13, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.8s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 14, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.8s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 15, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.8s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 16, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.8s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 17, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.9s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 18, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.9s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 19, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 0.9s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 20, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.0s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 21, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.1s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 22, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.1s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 23, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.1s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 24, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.2s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 25, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.2s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 26, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.2s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 27, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.3s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 28, current learner rf\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.4s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 29, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.4s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 30, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.4s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 31, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 32, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 33, current learner rf\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.5s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 34, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.6s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 35, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.6s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 36, current learner rf\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.7s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 37, current learner sgd\n[flaml.automl.logger: 08-16 07:13:36] {2466} INFO -  at 1.7s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:36] {2282} INFO - iteration 38, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 1.7s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 39, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 1.8s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 40, current learner rf\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 1.9s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 41, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.0s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 42, current learner sgd\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.0s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 43, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.0s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 44, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.1s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 45, current learner sgd\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.1s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 46, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.2s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 47, current learner rf\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.2s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 48, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.3s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 49, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.3s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 50, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.4s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 51, current learner sgd\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.5s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 52, current learner rf\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.6s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 53, current learner rf\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.6s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 54, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.7s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 55, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.7s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 56, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:37] {2466} INFO -  at 2.7s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:37] {2282} INFO - iteration 57, current learner rf\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 2.8s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 58, current learner rf\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 2.9s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 59, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 2.9s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 60, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.0s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 61, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.0s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 62, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.0s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 63, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.1s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 64, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.1s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 65, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.1s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 66, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.2s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 67, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.3s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 68, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.3s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 69, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.3s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 70, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.4s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 71, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.4s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 72, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.4s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 73, current learner sgd\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.5s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 74, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.5s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 75, current learner sgd\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.5s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 76, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.5s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 77, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.6s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 78, current learner sgd\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.6s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 79, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.6s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 80, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.7s,\testimator lgbm's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 81, current learner sgd\n[flaml.automl.logger: 08-16 07:13:38] {2466} INFO -  at 3.7s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:38] {2282} INFO - iteration 82, current learner rf\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 3.8s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 83, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 3.9s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 84, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 3.9s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 85, current learner sgd\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 3.9s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 86, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.0s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 87, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.1s,\testimator extra_tree's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 88, current learner rf\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.1s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 89, current learner rf\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.2s,\testimator rf's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 90, current learner sgd\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.2s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 91, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.3s,\testimator xgboost's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 92, current learner sgd\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.3s,\testimator sgd's best error=0.3402,\tbest estimator lgbm's best error=0.3402\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 93, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 94, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 95, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 96, current learner sgd\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 97, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 98, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 99, current learner rf\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.7s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 100, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:39] {2466} INFO -  at 4.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:39] {2282} INFO - iteration 101, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 4.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 102, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 4.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 103, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 4.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 104, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 4.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 105, current learner rf\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 4.9s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 106, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 107, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 108, current learner rf\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.1s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 109, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 110, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.2s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 111, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 112, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 113, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.3s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 114, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 115, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 116, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 117, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 118, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 119, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.5s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 120, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 121, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 122, current learner sgd\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 123, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 124, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:40] {2466} INFO -  at 5.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:40] {2282} INFO - iteration 125, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 5.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 126, current learner catboost\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.1s,\testimator catboost's best error=0.3422,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 127, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 128, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 129, current learner rf\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.3s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 130, current learner catboost\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.6s,\testimator catboost's best error=0.3422,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 131, current learner sgd\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 132, current learner sgd\n[flaml.automl.logger: 08-16 07:13:41] {2466} INFO -  at 6.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:41] {2282} INFO - iteration 133, current learner catboost\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 6.9s,\testimator catboost's best error=0.3422,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 134, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 6.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 135, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 136, current learner sgd\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 137, current learner catboost\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.3s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 138, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 139, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 140, current learner sgd\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 141, current learner rf\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.5s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 142, current learner sgd\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 143, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 144, current learner sgd\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 145, current learner sgd\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 146, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:42] {2466} INFO -  at 7.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:42] {2282} INFO - iteration 147, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 7.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 148, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 7.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 149, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 7.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 150, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 7.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 151, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 7.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 152, current learner rf\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.0s,\testimator rf's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 153, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 154, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.1s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 155, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 156, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 157, current learner catboost\n[flaml.automl.logger: 08-16 07:13:43] {2466} INFO -  at 8.7s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:43] {2282} INFO - iteration 158, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 8.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 159, current learner rf\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 8.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 160, current learner rf\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.0s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 161, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 162, current learner rf\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 163, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.2s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 164, current learner catboost\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.4s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 165, current learner sgd\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 166, current learner rf\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 167, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 168, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 169, current learner sgd\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 170, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:44] {2466} INFO -  at 9.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:44] {2282} INFO - iteration 171, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 9.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 172, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 9.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 173, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 9.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 174, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 9.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 175, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 9.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 176, current learner rf\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 177, current learner catboost\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.4s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 178, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 179, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 180, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 181, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 182, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 183, current learner rf\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.7s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 184, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:45] {2466} INFO -  at 10.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:45] {2282} INFO - iteration 185, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 10.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 186, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 10.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 187, current learner rf\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 10.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 188, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 189, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 190, current learner rf\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 191, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 192, current learner rf\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.3s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 193, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 194, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 195, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 196, current learner rf\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.5s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 197, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.5s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 198, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 199, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 200, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 201, current learner sgd\n[flaml.automl.logger: 08-16 07:13:46] {2466} INFO -  at 11.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:46] {2282} INFO - iteration 202, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 11.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 203, current learner rf\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 11.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 204, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 11.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 205, current learner catboost\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.2s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 206, current learner catboost\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.5s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 207, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 208, current learner sgd\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 209, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 210, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 211, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 212, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:47] {2466} INFO -  at 12.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:47] {2282} INFO - iteration 213, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 12.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 214, current learner sgd\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 12.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 215, current learner sgd\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 12.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 216, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 12.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 217, current learner catboost\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.1s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 218, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 219, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 220, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 221, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 222, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 223, current learner catboost\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.5s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 224, current learner sgd\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 225, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.5s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 226, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.6s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 227, current learner rf\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.7s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 228, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 229, current learner sgd\n[flaml.automl.logger: 08-16 07:13:48] {2466} INFO -  at 13.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:48] {2282} INFO - iteration 230, current learner rf\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.0s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 231, current learner sgd\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 232, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.2s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 233, current learner sgd\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 234, current learner sgd\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 235, current learner sgd\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 236, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 237, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 238, current learner sgd\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 239, current learner rf\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 240, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:49] {2466} INFO -  at 14.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:49] {2282} INFO - iteration 241, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 14.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 242, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 14.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 243, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 244, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 245, current learner sgd\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 246, current learner rf\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 247, current learner sgd\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 248, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 249, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 250, current learner sgd\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 251, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 252, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 253, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 254, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:50] {2466} INFO -  at 15.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:50] {2282} INFO - iteration 255, current learner sgd\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 15.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 256, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 15.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 257, current learner catboost\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.1s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 258, current learner rf\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.2s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 259, current learner catboost\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.5s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 260, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 261, current learner sgd\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 262, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:51] {2466} INFO -  at 16.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:51] {2282} INFO - iteration 263, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 16.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 264, current learner rf\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 16.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 265, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 16.9s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 266, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 17.1s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 267, current learner sgd\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 17.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 268, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 17.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 269, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 17.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 270, current learner sgd\n[flaml.automl.logger: 08-16 07:13:52] {2466} INFO -  at 17.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:52] {2282} INFO - iteration 271, current learner rf\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.0s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 272, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.1s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 273, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 274, current learner rf\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.2s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 275, current learner rf\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.3s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 276, current learner rf\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.4s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 277, current learner rf\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.5s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 278, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 279, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:53] {2466} INFO -  at 18.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:53] {2282} INFO - iteration 280, current learner catboost\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 18.9s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 281, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.1s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 282, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 283, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 284, current learner rf\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.3s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 285, current learner sgd\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 286, current learner sgd\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 287, current learner sgd\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 288, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 289, current learner rf\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 290, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.6s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 291, current learner sgd\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 292, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:54] {2466} INFO -  at 19.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:54] {2282} INFO - iteration 293, current learner sgd\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 19.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 294, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 19.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 295, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 296, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 297, current learner sgd\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 298, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 299, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 300, current learner rf\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.2s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 301, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 302, current learner rf\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.4s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 303, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 304, current learner catboost\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.7s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 305, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 306, current learner sgd\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:55] {2282} INFO - iteration 307, current learner sgd\n[flaml.automl.logger: 08-16 07:13:55] {2466} INFO -  at 20.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 308, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 309, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 310, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 311, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 312, current learner rf\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 313, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 314, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 315, current learner rf\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 316, current learner sgd\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 317, current learner sgd\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:56] {2282} INFO - iteration 318, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:56] {2466} INFO -  at 21.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 319, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 21.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 320, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 21.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 321, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 21.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 322, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 323, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 324, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 325, current learner sgd\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 326, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 327, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 328, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.3s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 329, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.3s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 330, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.3s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 331, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 332, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.4s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 333, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 334, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:57] {2466} INFO -  at 22.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:57] {2282} INFO - iteration 335, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 22.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 336, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 22.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 337, current learner sgd\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 22.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 338, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 22.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 339, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 22.9s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 340, current learner sgd\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 23.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 341, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 23.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 342, current learner rf\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 23.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 343, current learner sgd\n[flaml.automl.logger: 08-16 07:13:58] {2466} INFO -  at 23.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:58] {2282} INFO - iteration 344, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 23.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 345, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 23.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 346, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 23.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 347, current learner catboost\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.0s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 348, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 349, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 350, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 351, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.3s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 352, current learner extra_tree\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 353, current learner xgboost\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 354, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 355, current learner lgbm\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.6s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 356, current learner sgd\n[flaml.automl.logger: 08-16 07:13:59] {2466} INFO -  at 24.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:13:59] {2282} INFO - iteration 357, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:00] {2466} INFO -  at 24.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:00] {2282} INFO - iteration 358, current learner rf\n[flaml.automl.logger: 08-16 07:14:00] {2466} INFO -  at 25.2s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:00] {2282} INFO - iteration 359, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:00] {2466} INFO -  at 25.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:00] {2282} INFO - iteration 360, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:00] {2466} INFO -  at 25.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:00] {2282} INFO - iteration 361, current learner rf\n[flaml.automl.logger: 08-16 07:14:01] {2466} INFO -  at 26.0s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:01] {2282} INFO - iteration 362, current learner sgd\n[flaml.automl.logger: 08-16 07:14:01] {2466} INFO -  at 26.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:01] {2282} INFO - iteration 363, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:01] {2466} INFO -  at 26.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:01] {2282} INFO - iteration 364, current learner sgd\n[flaml.automl.logger: 08-16 07:14:01] {2466} INFO -  at 26.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:01] {2282} INFO - iteration 365, current learner catboost\n[flaml.automl.logger: 08-16 07:14:01] {2466} INFO -  at 26.5s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:01] {2282} INFO - iteration 366, current learner rf\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 26.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 367, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 26.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 368, current learner sgd\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 26.9s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 369, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 27.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 370, current learner catboost\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 27.2s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 371, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 27.5s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 372, current learner catboost\n[flaml.automl.logger: 08-16 07:14:02] {2466} INFO -  at 27.7s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:02] {2282} INFO - iteration 373, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 27.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 374, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 27.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 375, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 27.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 376, current learner rf\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 28.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 377, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 28.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 378, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 28.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 379, current learner sgd\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 28.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 380, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:03] {2466} INFO -  at 28.3s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:03] {2282} INFO - iteration 381, current learner rf\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 382, current learner sgd\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 383, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.3s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 384, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 385, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.5s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 386, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 387, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:04] {2466} INFO -  at 29.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:04] {2282} INFO - iteration 388, current learner catboost\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 29.8s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 389, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 29.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 390, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 29.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 391, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 29.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 392, current learner sgd\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 29.9s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 393, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 394, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 395, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 396, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 397, current learner sgd\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 398, current learner rf\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 399, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 400, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 401, current learner sgd\n[flaml.automl.logger: 08-16 07:14:05] {2466} INFO -  at 30.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:05] {2282} INFO - iteration 402, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 30.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 403, current learner sgd\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 30.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 404, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 30.9s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 405, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 30.9s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 406, current learner catboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.2s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 407, current learner sgd\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 408, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.2s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 409, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.4s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 410, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 411, current learner catboost\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.7s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 412, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:06] {2466} INFO -  at 31.7s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:06] {2282} INFO - iteration 413, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 31.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 414, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 31.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 415, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 31.8s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 416, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 31.8s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 417, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 418, current learner sgd\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 419, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.2s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 420, current learner sgd\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 421, current learner rf\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.6s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 422, current learner sgd\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 423, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 424, current learner sgd\n[flaml.automl.logger: 08-16 07:14:07] {2466} INFO -  at 32.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:07] {2282} INFO - iteration 425, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 32.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 426, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 32.9s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 427, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 428, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 429, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.0s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 430, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 431, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 432, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.1s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 433, current learner sgd\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 434, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 435, current learner sgd\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 436, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.3s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 437, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 438, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 439, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 440, current learner sgd\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 441, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 442, current learner sgd\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 443, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.6s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 444, current learner sgd\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 445, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:08] {2466} INFO -  at 33.6s,\testimator lgbm's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:08] {2282} INFO - iteration 446, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 33.8s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 447, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 33.9s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 448, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 33.9s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 449, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 33.9s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 450, current learner catboost\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.2s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 451, current learner sgd\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 452, current learner sgd\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 453, current learner sgd\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 454, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.4s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 455, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.5s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 456, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.5s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 457, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.7s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 458, current learner sgd\n[flaml.automl.logger: 08-16 07:14:09] {2466} INFO -  at 34.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:09] {2282} INFO - iteration 459, current learner catboost\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 34.9s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 460, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.1s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 461, current learner sgd\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 462, current learner sgd\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 463, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 464, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.3s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 465, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.3s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 466, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.3s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 467, current learner sgd\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 468, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.5s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 469, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.6s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 470, current learner sgd\n[flaml.automl.logger: 08-16 07:14:10] {2466} INFO -  at 35.6s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:10] {2282} INFO - iteration 471, current learner rf\n[flaml.automl.logger: 08-16 07:14:11] {2466} INFO -  at 36.2s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:11] {2282} INFO - iteration 472, current learner catboost\n[flaml.automl.logger: 08-16 07:14:11] {2466} INFO -  at 36.6s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:11] {2282} INFO - iteration 473, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:11] {2466} INFO -  at 36.6s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:11] {2282} INFO - iteration 474, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:11] {2466} INFO -  at 36.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:11] {2282} INFO - iteration 475, current learner rf\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.0s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 476, current learner sgd\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.0s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 477, current learner rf\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.5s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 478, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 479, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 480, current learner sgd\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.7s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 481, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:12] {2466} INFO -  at 37.7s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:12] {2282} INFO - iteration 482, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 37.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 483, current learner sgd\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 37.9s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 484, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 37.9s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 485, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 486, current learner rf\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.4s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 487, current learner sgd\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.4s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 488, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.4s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 489, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.6s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 490, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:13] {2466} INFO -  at 38.7s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:13] {2282} INFO - iteration 491, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 38.9s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 492, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.0s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 493, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.0s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 494, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 495, current learner rf\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.5s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 496, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 497, current learner sgd\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 498, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.6s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 499, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:14] {2466} INFO -  at 39.6s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:14] {2282} INFO - iteration 500, current learner rf\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.1s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 501, current learner sgd\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 502, current learner sgd\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.2s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 503, current learner sgd\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 504, current learner sgd\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.3s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 505, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.3s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 506, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.4s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 507, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:15] {2466} INFO -  at 40.4s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:15] {2282} INFO - iteration 508, current learner rf\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 40.9s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 509, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.0s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 510, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 511, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.0s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 512, current learner sgd\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.1s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 513, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.1s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 514, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.3s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 515, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.3s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 516, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:16] {2466} INFO -  at 41.3s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:16] {2282} INFO - iteration 517, current learner rf\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 41.8s,\testimator rf's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 518, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 41.8s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 519, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 41.8s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 520, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.0s,\testimator extra_tree's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 521, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 522, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.1s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 523, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.1s,\testimator lgbm's best error=0.3381,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 524, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.2s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 525, current learner catboost\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.5s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 526, current learner sgd\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.5s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 527, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:17] {2466} INFO -  at 42.5s,\testimator xgboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:17] {2282} INFO - iteration 528, current learner catboost\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 42.7s,\testimator catboost's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 529, current learner sgd\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 42.8s,\testimator sgd's best error=0.3402,\tbest estimator extra_tree's best error=0.3381\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 530, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 42.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 531, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 42.9s,\testimator xgb_limitdepth's best error=0.3873,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 532, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.0s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 533, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.0s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 534, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.1s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 535, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.2s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 536, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.2s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 537, current learner sgd\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 538, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.3s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 539, current learner sgd\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.4s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 540, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.5s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 541, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.5s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 542, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.6s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 543, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.7s,\testimator xgb_limitdepth's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 544, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:18] {2466} INFO -  at 43.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:18] {2282} INFO - iteration 545, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 43.8s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 546, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 43.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 547, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 43.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 548, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 549, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 550, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 551, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 552, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.1s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 553, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.2s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 554, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.2s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 555, current learner catboost\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.5s,\testimator catboost's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 556, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:19] {2466} INFO -  at 44.6s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:19] {2282} INFO - iteration 557, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 44.8s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 558, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 44.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 559, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 44.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 560, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 45.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 561, current learner rf\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 45.6s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 562, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 45.6s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 563, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 45.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 564, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:20] {2466} INFO -  at 45.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:20] {2282} INFO - iteration 565, current learner sgd\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 45.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 566, current learner rf\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.1s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 567, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.1s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 568, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.2s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 569, current learner sgd\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 570, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.3s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 571, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.3s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 572, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.4s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 573, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.5s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 574, current learner sgd\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.6s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 575, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.6s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 576, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:21] {2466} INFO -  at 46.7s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:21] {2282} INFO - iteration 577, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 46.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 578, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 46.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 579, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 46.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 580, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 46.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 581, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 582, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 583, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 584, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.2s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 585, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.3s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 586, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.3s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 587, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.3s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 588, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 589, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.4s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 590, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 591, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 592, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.7s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 593, current learner sgd\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.7s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 594, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:22] {2466} INFO -  at 47.7s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:22] {2282} INFO - iteration 595, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 47.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 596, current learner sgd\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 47.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 597, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 47.9s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 598, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 47.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 599, current learner sgd\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 47.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 600, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 601, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 602, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 603, current learner rf\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.5s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 604, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 605, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.6s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 606, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:23] {2466} INFO -  at 48.6s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:23] {2282} INFO - iteration 607, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 48.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 608, current learner sgd\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 48.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 609, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 48.9s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 610, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 48.9s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 611, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 612, current learner sgd\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.0s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 613, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 614, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 615, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.2s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 616, current learner sgd\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 617, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.3s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 618, current learner sgd\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.3s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 619, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.3s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 620, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:24] {2466} INFO -  at 49.5s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:24] {2282} INFO - iteration 621, current learner rf\n[flaml.automl.logger: 08-16 07:14:25] {2466} INFO -  at 50.0s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:25] {2282} INFO - iteration 622, current learner catboost\n[flaml.automl.logger: 08-16 07:14:25] {2466} INFO -  at 50.2s,\testimator catboost's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:25] {2282} INFO - iteration 623, current learner rf\n[flaml.automl.logger: 08-16 07:14:25] {2466} INFO -  at 50.7s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:25] {2282} INFO - iteration 624, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:25] {2466} INFO -  at 50.7s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:25] {2282} INFO - iteration 625, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 50.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 626, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 50.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 627, current learner rf\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.3s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 628, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.3s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 629, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 630, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.4s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 631, current learner sgd\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.4s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 632, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 633, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.5s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 634, current learner sgd\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 635, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.6s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 636, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:26] {2466} INFO -  at 51.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:26] {2282} INFO - iteration 637, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.7s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 638, current learner sgd\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 639, current learner sgd\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 640, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 641, current learner sgd\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 642, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.9s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 643, current learner sgd\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 51.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 644, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 645, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 646, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 647, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.1s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 648, current learner sgd\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 649, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.2s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 650, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.2s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 651, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:27] {2466} INFO -  at 52.4s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:27] {2282} INFO - iteration 652, current learner rf\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.0s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 653, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 654, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.0s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 655, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.2s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 656, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.2s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 657, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.3s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 658, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.4s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 659, current learner sgd\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.4s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 660, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.4s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 661, current learner catboost\n[flaml.automl.logger: 08-16 07:14:28] {2466} INFO -  at 53.7s,\testimator catboost's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:28] {2282} INFO - iteration 662, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 53.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 663, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 53.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 664, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 53.9s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 665, current learner sgd\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.1s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 666, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 667, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 668, current learner sgd\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 669, current learner rf\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.6s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 670, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.6s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 671, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.7s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 672, current learner sgd\n[flaml.automl.logger: 08-16 07:14:29] {2466} INFO -  at 54.7s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:29] {2282} INFO - iteration 673, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 54.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 674, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 54.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 675, current learner sgd\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 54.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 676, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 54.9s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 677, current learner sgd\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 54.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 678, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 679, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.0s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 680, current learner sgd\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.1s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 681, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 682, current learner catboost\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.4s,\testimator catboost's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 683, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 684, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.5s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 685, current learner sgd\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 686, current learner sgd\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 687, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:30] {2466} INFO -  at 55.6s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:30] {2282} INFO - iteration 688, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 55.8s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 689, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 55.9s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 690, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 55.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 691, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 55.9s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 692, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 55.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 693, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.0s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 694, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.0s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 695, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 696, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 697, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.2s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 698, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.2s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 699, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.3s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 700, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 701, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.4s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 702, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 703, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 704, current learner sgd\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 705, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:31] {2466} INFO -  at 56.7s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:31] {2282} INFO - iteration 706, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 56.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 707, current learner rf\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.1s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 708, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.1s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 709, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.2s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 710, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.2s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 711, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.3s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 712, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.3s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 713, current learner sgd\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.3s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 714, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 715, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 716, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.5s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 717, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 718, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 719, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.6s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 720, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 721, current learner sgd\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.7s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 722, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:32] {2466} INFO -  at 57.7s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:32] {2282} INFO - iteration 723, current learner sgd\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 57.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 724, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 57.8s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 725, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 57.8s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 726, current learner rf\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 58.4s,\testimator rf's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 727, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 58.5s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 728, current learner sgd\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 58.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 729, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 58.5s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 730, current learner sgd\n[flaml.automl.logger: 08-16 07:14:33] {2466} INFO -  at 58.5s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:33] {2282} INFO - iteration 731, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 58.8s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 732, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 58.9s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 733, current learner sgd\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.0s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 734, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.0s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 735, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.1s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 736, current learner sgd\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.1s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 737, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.3s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 738, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 739, current learner sgd\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.4s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 740, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.4s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 741, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.5s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 742, current learner extra_tree\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.6s,\testimator extra_tree's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 743, current learner sgd\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.6s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 744, current learner xgb_limitdepth\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.7s,\testimator xgb_limitdepth's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 745, current learner sgd\n[flaml.automl.logger: 08-16 07:14:34] {2466} INFO -  at 59.7s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:34] {2282} INFO - iteration 746, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 59.8s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 747, current learner sgd\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 59.8s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 748, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 59.9s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 749, current learner xgboost\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 59.9s,\testimator xgboost's best error=0.3361,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 750, current learner sgd\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 59.9s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 751, current learner lgbm\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 60.0s,\testimator lgbm's best error=0.3381,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2282} INFO - iteration 752, current learner sgd\n[flaml.automl.logger: 08-16 07:14:35] {2466} INFO -  at 60.0s,\testimator sgd's best error=0.3402,\tbest estimator xgboost's best error=0.3361\n[flaml.automl.logger: 08-16 07:14:35] {2724} INFO - retrain xgboost for 0.0s\n[flaml.automl.logger: 08-16 07:14:35] {2727} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n              colsample_bylevel=0.9726845752023129, colsample_bynode=None,\n              colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy='lossguide', importance_type=None,\n              interaction_constraints=None, learning_rate=0.28039711606062834,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=0, max_leaves=8,\n              min_child_weight=0.25633619672240004, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=4,\n              n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n[flaml.automl.logger: 08-16 07:14:35] {2009} INFO - fit succeeded\n[flaml.automl.logger: 08-16 07:14:35] {2010} INFO - Time taken to find the best model: 42.82643675804138\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# Predict class labels\ny_pred = automl.predict(X_test)\n\n# Predict probabilities (for classification)\ny_pred_proba = automl.predict_proba(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:14:56.095495Z","iopub.execute_input":"2025-08-16T07:14:56.095773Z","iopub.status.idle":"2025-08-16T07:14:56.126734Z","shell.execute_reply.started":"2025-08-16T07:14:56.095756Z","shell.execute_reply":"2025-08-16T07:14:56.125926Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"print(automl.model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:15:02.002930Z","iopub.execute_input":"2025-08-16T07:15:02.003269Z","iopub.status.idle":"2025-08-16T07:15:02.007922Z","shell.execute_reply.started":"2025-08-16T07:15:02.003247Z","shell.execute_reply":"2025-08-16T07:15:02.007160Z"}},"outputs":[{"name":"stdout","text":"<flaml.automl.model.XGBoostSklearnEstimator object at 0x789fdf730390>\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Step 1: Get predicted probabilities (already done)\n# y_pred_proba = automl.predict_proba(X_test)\n\n# Step 2: Apply threshold to get binary labels\nthreshold = 0.3\ny_pred = (y_pred_proba[:, 1] >= threshold).astype(int)  # For binary classification\n\n# Step 3: Print the classification metrics\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:17:49.898795Z","iopub.execute_input":"2025-08-16T07:17:49.899098Z","iopub.status.idle":"2025-08-16T07:17:49.913486Z","shell.execute_reply.started":"2025-08-16T07:17:49.899074Z","shell.execute_reply":"2025-08-16T07:17:49.912871Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         0.0     0.8828    0.1409    0.2430       802\n         1.0     0.1301    0.8729    0.2264       118\n\n    accuracy                         0.2348       920\n   macro avg     0.5064    0.5069    0.2347       920\nweighted avg     0.7863    0.2348    0.2409       920\n\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"code","source":"len(X_train_tab_scaled)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:23:03.034401Z","iopub.execute_input":"2025-08-16T07:23:03.034723Z","iopub.status.idle":"2025-08-16T07:23:03.039893Z","shell.execute_reply.started":"2025-08-16T07:23:03.034701Z","shell.execute_reply":"2025-08-16T07:23:03.039280Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"4855"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"# ==============================\n# Weighted-block Stacking w/ Optuna\n# Base learners: XGB, CatBoost, Logistic Regression\n# Meta learner: Logistic Regression\n# ==============================\n\nimport numpy as np\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score  # change to your metric if needed\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# --------------------------------------------------\n# Required inputs \n# Shapes: (n_train, d_*), (n_test, d_*)\n# --------------------------------------------------\n# X_train_effnet, X_test_effnet\n# X_train_resnet, X_test_resnet\n# X_train_vit_im,   X_test_vit_im\n# X_train_tab_scaled,    X_test_tab_scaled\n# y_train_,     y_test_\n\n# ====== Helpers ======\n\ndef weighted_concat(blocks, weights):\n    \"\"\"Multiply each feature block by its scalar weight and concatenate horizontally.\"\"\"\n    assert len(blocks) == len(weights)\n    w_blocks = [w * b for w, b in zip(weights, blocks)]\n    return np.hstack(w_blocks)\n\ndef get_oof_preds(model, X, y, X_test, n_splits=5, random_state=42):\n    \"\"\"Return OOF predictions for stacking + mean test predictions.\"\"\"\n    oof = np.zeros(len(X))\n    test_fold = np.zeros((len(X_test), n_splits))\n\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X[tr_idx], y[tr_idx]\n        X_va, y_va = X[va_idx], y[va_idx]\n\n        model.fit(X_tr, y_tr)\n        oof[va_idx] = model.predict_proba(X_va)[:, 1]\n        test_fold[:, fold] = model.predict_proba(X_test)[:, 1]\n\n    return oof.reshape(-1, 1), test_fold.mean(axis=1).reshape(-1, 1)\n\n# ====== Optuna Objective ======\n\ndef objective(trial):\n    # -----------------------------\n    # 1) Suggest weights in [1, 5]\n    # -----------------------------\n    # XGB input weights\n    a1 = trial.suggest_float(\"xgb_w_effnet\", 1.0, 5.0)\n    a2 = trial.suggest_float(\"xgb_w_resnet\", 1.0, 5.0)\n    a3 = trial.suggest_float(\"xgb_w_vit\",    1.0, 5.0)\n    a4 = trial.suggest_float(\"xgb_w_tab\",    1.0, 5.0)\n\n    # CatBoost input weights\n    b1 = trial.suggest_float(\"cat_w_effnet\", 1.0, 5.0)\n    b2 = trial.suggest_float(\"cat_w_resnet\", 1.0, 5.0)\n    b3 = trial.suggest_float(\"cat_w_vit\",    1.0, 5.0)\n    b4 = trial.suggest_float(\"cat_w_tab\",    1.0, 5.0)\n\n    # LR input weights\n    c1 = trial.suggest_float(\"lr_w_effnet\",  1.0, 5.0)\n    c2 = trial.suggest_float(\"lr_w_resnet\",  1.0, 5.0)\n    c3 = trial.suggest_float(\"lr_w_vit\",     1.0, 5.0)\n    c4 = trial.suggest_float(\"lr_w_tab\",     1.0, 5.0)\n\n    # -----------------------------\n    # 2) Suggest model hyperparams\n    # -----------------------------\n    # XGB\n    xgb_params = {\n        \"n_estimators\": 300,\n        \"max_depth\": 4,\n        \"learning_rate\": 0.05,\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.7,\n        \"reg_lambda\": 2e-3,\n        \"random_state\": 42,\n        \"eval_metric\": \"logloss\",\n        \"use_label_encoder\": False,\n        \"n_jobs\": -1,\n    }\n\n    # CatBoost (use silent training)\n    cat_params = {\n        \"iterations\": 500,\n        \"depth\": 5,\n        \"learning_rate\": 2e-3,\n        \"l2_leaf_reg\": 3,\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"AUC\",\n        \"random_seed\": 42,\n        \"verbose\": False\n    }\n\n    # LR (base learners & meta learner use possibly different Cs)\n    lr_C_base = trial.suggest_float(\"lr_base_C\", 1e-3, 10.0, log=True)\n    lr_solver_base = trial.suggest_categorical(\"lr_base_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n\n    lr_C_meta = trial.suggest_float(\"lr_meta_C\", 1e-3, 10.0, log=True)\n    lr_solver_meta = trial.suggest_categorical(\"lr_meta_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n\n    # -----------------------------\n    # 3) Build weighted inputs\n    # -----------------------------\n    X_xgb_train = weighted_concat(\n        [X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n        [a1, a2, a3, a4]\n    )\n    X_xgb_test = weighted_concat(\n        [X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n        [a1, a2, a3, a4]\n    )\n\n    X_cat_train = weighted_concat(\n        [X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n        [b1, b2, b3, b4]\n    )\n    X_cat_test = weighted_concat(\n        [X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n        [b1, b2, b3, b4]\n    )\n\n    X_lr_train = weighted_concat(\n        [X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n        [c1, c2, c3, c4]\n    )\n    X_lr_test = weighted_concat(\n        [X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n        [c1, c2, c3, c4]\n    )\n\n    # -----------------------------\n    # 4) Define base learners\n    # -----------------------------\n    base_xgb = XGBClassifier(**xgb_params)\n    base_cat = CatBoostClassifier(**cat_params)\n    base_lr  = LogisticRegression(\n        C=lr_C_base, solver=lr_solver_base, max_iter=1000, n_jobs=None if lr_solver_base==\"liblinear\" else -1\n    )\n\n    # -----------------------------\n    # 5) OOF predictions for stacking\n    # -----------------------------\n    oof_xgb, test_xgb = get_oof_preds(base_xgb, X_xgb_train, y_train_, X_xgb_test)\n    oof_cat, test_cat = get_oof_preds(base_cat, X_cat_train, y_train_, X_cat_test)\n    oof_lr,  test_lr  = get_oof_preds(base_lr,  X_lr_train,  y_train_, X_lr_test)\n\n    # Stack into meta features\n    meta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\n    meta_test  = np.hstack([test_xgb, test_cat, test_lr])\n\n    # -----------------------------\n    # 6) Meta learner (LR)\n    # -----------------------------\n    meta_model = LogisticRegression(\n        C=lr_C_meta, solver=lr_solver_meta, max_iter=1000,\n        n_jobs=None if lr_solver_meta==\"liblinear\" else -1\n    )\n    meta_model.fit(meta_train, y_train_)\n    meta_probs = meta_model.predict_proba(meta_test)[:, 1]\n\n    # -----------------------------\n    # 7) Metric (AUC by default)\n    #     - If you want recall of minority class instead:\n    #         from sklearn.metrics import recall_score\n    #         preds = (meta_probs >= 0.5).astype(int)\n    #         return recall_score(y_test_, preds, pos_label=1)\n    # -----------------------------\n    preds = (meta_probs >= 0.3).astype(int)  # Threshold at 0.3\n    recall = recall_score(y_test_, preds, pos_label=1)\n    return recall\n\n# ====== Run Optuna ======\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)   # increase n_trials for better search\n\nprint(\"Best AUC:\", study.best_value)\nprint(\"Best params:\")\nfor k, v in study.best_params.items():\n    print(f\"  {k}: {v}\")\n\n# ====== Train final model with best params ======\nbest = study.best_params\n\n# Rebuild weights\na1, a2, a3, a4 = best[\"xgb_w_effnet\"], best[\"xgb_w_resnet\"], best[\"xgb_w_vit\"], best[\"xgb_w_tab\"]\nb1, b2, b3, b4 = best[\"cat_w_effnet\"], best[\"cat_w_resnet\"], best[\"cat_w_vit\"], best[\"cat_w_tab\"]\nc1, c2, c3, c4 = best[\"lr_w_effnet\"],  best[\"lr_w_resnet\"],  best[\"lr_w_vit\"],  best[\"lr_w_tab\"]\n\n# Rebuild features\nX_xgb_train = weighted_concat([effnet_train, resnet_train, vit_train, tab_train], [a1, a2, a3, a4])\nX_xgb_test  = weighted_concat([effnet_test,  resnet_test,  vit_test,  tab_test],  [a1, a2, a3, a4])\n\nX_cat_train = weighted_concat([effnet_train, resnet_train, vit_train, tab_train], [b1, b2, b3, b4])\nX_cat_test  = weighted_concat([effnet_test,  resnet_test,  vit_test,  tab_test],  [b1, b2, b3, b4])\n\nX_lr_train  = weighted_concat([effnet_train, resnet_train, vit_train, tab_train], [c1, c2, c3, c4])\nX_lr_test   = weighted_concat([effnet_test,  resnet_test,  vit_test,  tab_test],  [c1, c2, c3, c4])\n\n# Final models\nfinal_xgb = XGBClassifier(\n    n_estimators=best[\"xgb_n_estimators\"],\n    max_depth=best[\"xgb_max_depth\"],\n    learning_rate=best[\"xgb_lr\"],\n    subsample=best[\"xgb_subsample\"],\n    colsample_bytree=best[\"xgb_colsample\"],\n    reg_lambda=best[\"xgb_reg_lambda\"],\n    random_state=42, eval_metric=\"logloss\", use_label_encoder=False, n_jobs=-1\n)\nfinal_cat = CatBoostClassifier(\n    iterations=best[\"cat_iterations\"], depth=best[\"cat_depth\"],\n    learning_rate=best[\"cat_lr\"], l2_leaf_reg=best[\"cat_l2_leaf_reg\"],\n    loss_function=\"Logloss\", eval_metric=\"AUC\", random_seed=42, verbose=False\n)\nfinal_lr_base = LogisticRegression(\n    C=best[\"lr_base_C\"], solver=best[\"lr_base_solver\"], max_iter=1000,\n    n_jobs=None if best[\"lr_base_solver\"]==\"liblinear\" else -1\n)\n\n# OOF again to build full meta features (train on all folds)\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr,  test_lr  = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test  = np.hstack([test_xgb, test_cat, test_lr])\n\nfinal_meta = LogisticRegression(\n    C=best[\"lr_meta_C\"], solver=best[\"lr_meta_solver\"], max_iter=1000,\n    n_jobs=None if best[\"lr_meta_solver\"]==\"liblinear\" else -1\n)\nfinal_meta.fit(meta_train, y_train_)\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\n\n# Report\nfrom sklearn.metrics import roc_auc_score, classification_report\nfinal_auc = roc_auc_score(y_test_, final_probs)\nfinal_preds = (final_probs >= 0.3).astype(int)\nprint(f\"\\nFinal AUC: {final_auc:.4f}\")\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:43:52.540776Z","iopub.execute_input":"2025-08-16T07:43:52.541740Z","iopub.status.idle":"2025-08-16T07:44:38.959143Z","shell.execute_reply.started":"2025-08-16T07:43:52.541709Z","shell.execute_reply":"2025-08-16T07:44:38.958119Z"}},"outputs":[{"name":"stderr","text":"[I 2025-08-16 07:43:52,564] A new study created in memory with name: no-name-b52bba5d-3cf3-4c56-831e-6647f74f3189\n[W 2025-08-16 07:44:38,921] Trial 0 failed with parameters: {'xgb_w_effnet': 4.660558757654176, 'xgb_w_resnet': 1.026490307067438, 'xgb_w_vit': 4.621715800737713, 'xgb_w_tab': 1.2590703130942704, 'cat_w_effnet': 4.049580052978702, 'cat_w_resnet': 4.598132819712559, 'cat_w_vit': 1.2712661756691355, 'cat_w_tab': 3.0810582848374977, 'lr_w_effnet': 3.254347329143679, 'lr_w_resnet': 1.247330012159305, 'lr_w_vit': 1.3038227691581863, 'lr_w_tab': 4.968181531825035, 'lr_base_C': 1.1874525731286782, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.14354474451353696, 'lr_meta_solver': 'saga'} because of the following error: NameError(\"name 'recall_score' is not defined\").\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_629/4247686215.py\", line 177, in objective\n    recall = recall_score(y_test_, preds, pos_label=1)\n             ^^^^^^^^^^^^\nNameError: name 'recall_score' is not defined\n[W 2025-08-16 07:44:38,922] Trial 0 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_629/4247686215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# ====== Run Optuna ======\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# increase n_trials for better search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best AUC:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_629/4247686215.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmeta_probs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Threshold at 0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'recall_score' is not defined"],"ename":"NameError","evalue":"name 'recall_score' is not defined","output_type":"error"}],"execution_count":89},{"cell_type":"code","source":"best = study.best_params  # Already done after running Optuna\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:35:27.343553Z","iopub.execute_input":"2025-08-16T07:35:27.344369Z","iopub.status.idle":"2025-08-16T07:35:27.348679Z","shell.execute_reply.started":"2025-08-16T07:35:27.344339Z","shell.execute_reply":"2025-08-16T07:35:27.347857Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"X_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], \n                              [best[\"xgb_w_effnet\"], best[\"xgb_w_resnet\"], best[\"xgb_w_vit\"], best[\"xgb_w_tab\"]])\nX_xgb_test  = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], \n                              [best[\"xgb_w_effnet\"], best[\"xgb_w_resnet\"], best[\"xgb_w_vit\"], best[\"xgb_w_tab\"]])\n\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], \n                              [best[\"cat_w_effnet\"], best[\"cat_w_resnet\"], best[\"cat_w_vit\"], best[\"cat_w_tab\"]])\nX_cat_test  = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], \n                              [best[\"cat_w_effnet\"], best[\"cat_w_resnet\"], best[\"cat_w_vit\"], best[\"cat_w_tab\"]])\n\nX_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], \n                             [best[\"lr_w_effnet\"], best[\"lr_w_resnet\"], best[\"lr_w_vit\"], best[\"lr_w_tab\"]])\nX_lr_test  = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], \n                             [best[\"lr_w_effnet\"], best[\"lr_w_resnet\"], best[\"lr_w_vit\"], best[\"lr_w_tab\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:35:32.918470Z","iopub.execute_input":"2025-08-16T07:35:32.919074Z","iopub.status.idle":"2025-08-16T07:35:32.993383Z","shell.execute_reply.started":"2025-08-16T07:35:32.919051Z","shell.execute_reply":"2025-08-16T07:35:32.992765Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"final_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    n_jobs=-1\n)\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    random_seed=42,\n    verbose=False\n)\nfinal_lr_base = LogisticRegression(\n    C=best[\"lr_base_C\"], \n    solver=best[\"lr_base_solver\"], \n    max_iter=1000, \n    n_jobs=None if best[\"lr_base_solver\"]==\"liblinear\" else -1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:35:41.451609Z","iopub.execute_input":"2025-08-16T07:35:41.452362Z","iopub.status.idle":"2025-08-16T07:35:41.457089Z","shell.execute_reply.started":"2025-08-16T07:35:41.452336Z","shell.execute_reply":"2025-08-16T07:35:41.456489Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"oof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr,  test_lr  = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test  = np.hstack([test_xgb, test_cat, test_lr])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:35:49.383692Z","iopub.execute_input":"2025-08-16T07:35:49.384250Z","iopub.status.idle":"2025-08-16T07:36:44.672745Z","shell.execute_reply.started":"2025-08-16T07:35:49.384228Z","shell.execute_reply":"2025-08-16T07:36:44.672122Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"final_meta = LogisticRegression(\n    C=best[\"lr_meta_C\"], solver=best[\"lr_meta_solver\"], max_iter=1000,\n    n_jobs=None if best[\"lr_meta_solver\"]==\"liblinear\" else -1\n)\nfinal_meta.fit(meta_train, y_train_)\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:36:49.618595Z","iopub.execute_input":"2025-08-16T07:36:49.618883Z","iopub.status.idle":"2025-08-16T07:36:50.399747Z","shell.execute_reply.started":"2025-08-16T07:36:49.618863Z","shell.execute_reply":"2025-08-16T07:36:50.398977Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, classification_report\nfinal_auc = roc_auc_score(y_test_, final_probs)\nfinal_preds = (final_probs >= 0.3).astype(int)\nprint(f\"\\nFinal AUC: {final_auc:.4f}\")\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:37:01.003502Z","iopub.execute_input":"2025-08-16T07:37:01.004048Z","iopub.status.idle":"2025-08-16T07:37:01.019548Z","shell.execute_reply.started":"2025-08-16T07:37:01.004022Z","shell.execute_reply":"2025-08-16T07:37:01.018808Z"}},"outputs":[{"name":"stdout","text":"\nFinal AUC: 0.9204\n              precision    recall  f1-score   support\n\n         0.0     0.9496    0.9626    0.9560       802\n         1.0     0.7196    0.6525    0.6844       118\n\n    accuracy                         0.9228       920\n   macro avg     0.8346    0.8076    0.8202       920\nweighted avg     0.9201    0.9228    0.9212       920\n\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"import numpy as np\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, roc_auc_score, classification_report\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# --------------------------------------------------\n# Required inputs (replace with your data arrays)\n# Shapes: (n_train, d_*), (n_test, d_*)\n# --------------------------------------------------\n# X_train_effnet, X_test_effnet\n# X_train_resnet, X_test_resnet\n# X_train_vit_im,   X_test_vit_im\n# X_train_tab_scaled,    X_test_tab_scaled\n# y_train_,     y_test_\n# --------------------------------------------------\n\ndef weighted_concat(blocks, weights):\n    \"\"\"Multiply each feature block by its scalar weight and concatenate horizontally.\"\"\"\n    assert len(blocks) == len(weights)\n    w_blocks = [w * b for w, b in zip(weights, blocks)]\n    return np.hstack(w_blocks)\n\ndef get_oof_preds(model, X, y, X_test, n_splits=5, random_state=42):\n    \"\"\"Return OOF predictions for stacking + mean test predictions.\"\"\"\n    oof = np.zeros(len(X))\n    test_fold = np.zeros((len(X_test), n_splits))\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X[tr_idx], y[tr_idx]\n        X_va, y_va = X[va_idx], y[va_idx]\n        model.fit(X_tr, y_tr)\n        oof[va_idx] = model.predict_proba(X_va)[:, 1]\n        test_fold[:, fold] = model.predict_proba(X_test)[:, 1]\n    return oof.reshape(-1, 1), test_fold.mean(axis=1).reshape(-1, 1)\n\ndef objective(trial):\n    # Suggest weights for each feature block for each model\n    a1 = trial.suggest_float(\"xgb_w_effnet\", 1.0, 5.0)\n    a2 = trial.suggest_float(\"xgb_w_resnet\", 1.0, 5.0)\n    a3 = trial.suggest_float(\"xgb_w_vit\", 1.0, 5.0)\n    a4 = trial.suggest_float(\"xgb_w_tab\", 1.0, 5.0)\n\n    b1 = trial.suggest_float(\"cat_w_effnet\", 1.0, 5.0)\n    b2 = trial.suggest_float(\"cat_w_resnet\", 1.0, 5.0)\n    b3 = trial.suggest_float(\"cat_w_vit\", 1.0, 5.0)\n    b4 = trial.suggest_float(\"cat_w_tab\", 1.0, 5.0)\n\n    c1 = trial.suggest_float(\"lr_w_effnet\", 1.0, 5.0)\n    c2 = trial.suggest_float(\"lr_w_resnet\", 1.0, 5.0)\n    c3 = trial.suggest_float(\"lr_w_vit\", 1.0, 5.0)\n    c4 = trial.suggest_float(\"lr_w_tab\", 1.0, 5.0)\n\n    # Suggest hyperparameters for logistic regression base and meta learners\n    lr_C_base = trial.suggest_float(\"lr_base_C\", 1e-3, 10.0, log=True)\n    lr_solver_base = trial.suggest_categorical(\"lr_base_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n    lr_C_meta = trial.suggest_float(\"lr_meta_C\", 1e-3, 10.0, log=True)\n    lr_solver_meta = trial.suggest_categorical(\"lr_meta_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n\n    # XGB fixed params\n    xgb_params = {\n        \"n_estimators\": 300,\n        \"max_depth\": 4,\n        \"learning_rate\": 0.05,\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.7,\n        \"reg_lambda\": 2e-3,\n        \"random_state\": 42,\n        \"eval_metric\": \"logloss\",\n        \"use_label_encoder\": False,\n        \"n_jobs\": -1,\n    }\n\n    # CatBoost fixed params\n    cat_params = {\n        \"iterations\": 500,\n        \"depth\": 5,\n        \"learning_rate\": 2e-3,\n        \"l2_leaf_reg\": 3,\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"AUC\",\n        \"random_seed\": 42,\n        \"verbose\": False\n    }\n\n    # Build weighted inputs for base models\n    X_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [a1, a2, a3, a4])\n    X_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [a1, a2, a3, a4])\n\n    X_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [b1, b2, b3, b4])\n    X_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [b1, b2, b3, b4])\n\n    X_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [c1, c2, c3, c4])\n    X_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [c1, c2, c3, c4])\n\n    # Initialize base learners\n    base_xgb = XGBClassifier(**xgb_params)\n    base_cat = CatBoostClassifier(**cat_params)\n    base_lr = LogisticRegression(\n        C=lr_C_base, solver=lr_solver_base,\n        max_iter=1000,\n        n_jobs=None if lr_solver_base == \"liblinear\" else -1\n    )\n\n    # Get out-of-fold predictions and test predictions\n    oof_xgb, test_xgb = get_oof_preds(base_xgb, X_xgb_train, y_train_, X_xgb_test)\n    oof_cat, test_cat = get_oof_preds(base_cat, X_cat_train, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:45:34.845909Z","iopub.execute_input":"2025-08-16T07:45:34.846705Z","iopub.status.idle":"2025-08-16T07:45:34.859522Z","shell.execute_reply.started":"2025-08-16T07:45:34.846676Z","shell.execute_reply":"2025-08-16T07:45:34.858615Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_629/1610422388.py\"\u001b[0;36m, line \u001b[0;32m109\u001b[0m\n\u001b[0;31m    oof_cat, test_cat = get_oof_preds(base_cat, X_cat_train, y\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (1610422388.py, line 109)","output_type":"error"}],"execution_count":90},{"cell_type":"code","source":"import numpy as np\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, roc_auc_score, classification_report\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# --------------------------------------------------\n# Required inputs (replace with your data arrays)\n# Shapes: (n_train, d_*), (n_test, d_*)\n# --------------------------------------------------\n# X_train_effnet, X_test_effnet\n# X_train_resnet, X_test_resnet\n# X_train_vit_im,   X_test_vit_im\n# X_train_tab_scaled,    X_test_tab_scaled\n# y_train_,     y_test_\n# --------------------------------------------------\n\ndef weighted_concat(blocks, weights):\n    \"\"\"Multiply each feature block by its scalar weight and concatenate horizontally.\"\"\"\n    assert len(blocks) == len(weights)\n    w_blocks = [w * b for w, b in zip(weights, blocks)]\n    return np.hstack(w_blocks)\n\ndef get_oof_preds(model, X, y, X_test, n_splits=5, random_state=42):\n    \"\"\"Return OOF predictions for stacking + mean test predictions.\"\"\"\n    oof = np.zeros(len(X))\n    test_fold = np.zeros((len(X_test), n_splits))\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X[tr_idx], y[tr_idx]\n        X_va, y_va = X[va_idx], y[va_idx]\n        model.fit(X_tr, y_tr)\n        oof[va_idx] = model.predict_proba(X_va)[:, 1]\n        test_fold[:, fold] = model.predict_proba(X_test)[:, 1]\n    return oof.reshape(-1, 1), test_fold.mean(axis=1).reshape(-1, 1)\n\ndef objective(trial):\n    # Suggest weights for each feature block for each model\n    a1 = trial.suggest_float(\"xgb_w_effnet\", 1.0, 5.0)\n    a2 = trial.suggest_float(\"xgb_w_resnet\", 1.0, 5.0)\n    a3 = trial.suggest_float(\"xgb_w_vit\", 1.0, 5.0)\n    a4 = trial.suggest_float(\"xgb_w_tab\", 1.0, 5.0)\n\n    b1 = trial.suggest_float(\"cat_w_effnet\", 1.0, 5.0)\n    b2 = trial.suggest_float(\"cat_w_resnet\", 1.0, 5.0)\n    b3 = trial.suggest_float(\"cat_w_vit\", 1.0, 5.0)\n    b4 = trial.suggest_float(\"cat_w_tab\", 1.0, 5.0)\n\n    c1 = trial.suggest_float(\"lr_w_effnet\", 1.0, 5.0)\n    c2 = trial.suggest_float(\"lr_w_resnet\", 1.0, 5.0)\n    c3 = trial.suggest_float(\"lr_w_vit\", 1.0, 5.0)\n    c4 = trial.suggest_float(\"lr_w_tab\", 1.0, 5.0)\n\n    # Suggest hyperparameters for logistic regression base and meta learners\n    lr_C_base = trial.suggest_float(\"lr_base_C\", 1e-3, 10.0, log=True)\n    lr_solver_base = trial.suggest_categorical(\"lr_base_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n    lr_C_meta = trial.suggest_float(\"lr_meta_C\", 1e-3, 10.0, log=True)\n    lr_solver_meta = trial.suggest_categorical(\"lr_meta_solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n\n    # XGB fixed params\n    xgb_params = {\n        \"n_estimators\": 300,\n        \"max_depth\": 4,\n        \"learning_rate\": 0.05,\n        \"subsample\": 0.85,\n        \"colsample_bytree\": 0.7,\n        \"reg_lambda\": 2e-3,\n        \"random_state\": 42,\n        \"eval_metric\": \"logloss\",\n        \"use_label_encoder\": False,\n        \"n_jobs\": -1,\n    }\n\n    # CatBoost fixed params\n    cat_params = {\n        \"iterations\": 500,\n        \"depth\": 5,\n        \"learning_rate\": 2e-3,\n        \"l2_leaf_reg\": 3,\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"AUC\",\n        \"random_seed\": 42,\n        \"verbose\": False\n    }\n\n    # Build weighted inputs for base models\n    X_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [a1, a2, a3, a4])\n    X_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [a1, a2, a3, a4])\n\n    X_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [b1, b2, b3, b4])\n    X_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [b1, b2, b3, b4])\n\n    X_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [c1, c2, c3, c4])\n    X_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [c1, c2, c3, c4])\n\n    # Initialize base learners\n    base_xgb = XGBClassifier(**xgb_params)\n    base_cat = CatBoostClassifier(**cat_params)\n    base_lr = LogisticRegression(\n        C=lr_C_base, solver=lr_solver_base,\n        max_iter=1000,\n        n_jobs=None if lr_solver_base == \"liblinear\" else -1\n    )\n\n    # Get out-of-fold predictions and test predictions\n    oof_xgb, test_xgb = get_oof_preds(base_xgb, X_xgb_train, y_train_, X_xgb_test)\n    oof_cat, test_cat = get_oof_preds(base_cat, X_cat_train, y_train_, X_cat_test)\n    oof_lr, test_lr = get_oof_preds(base_lr, X_lr_train, y_train_, X_lr_test)\n\n    # Stack predictions\n    meta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\n    meta_test = np.hstack([test_xgb, test_cat, test_lr])\n\n    # Train meta-learner\n    meta_model = LogisticRegression(\n        C=lr_C_meta,\n        solver=lr_solver_meta,\n        max_iter=1000,\n        n_jobs=None if lr_solver_meta == \"liblinear\" else -1\n    )\n    meta_model.fit(meta_train, y_train_)\n\n    # Predict probabilities and compute recall at threshold=0.3\n    meta_probs = meta_model.predict_proba(meta_test)[:, 1]\n    threshold = 0.3\n    meta_preds = (meta_probs >= threshold).astype(int)\n    recall = recall_score(y_test_, meta_preds, pos_label=1)\n    return recall\n\n# Run Optuna optimization\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Best recall:\", study.best_value)\nprint(\"Best parameters:\")\nfor key, value in study.best_params.items():\n    print(f\"  {key}: {value}\")\n\n# Train final model with best parameters\nbest = study.best_params\n\n# Reconstruct weighted feature sets with best weights\na1, a2, a3, a4 = best[\"xgb_w_effnet\"], best[\"xgb_w_resnet\"], best[\"xgb_w_vit\"], best[\"xgb_w_tab\"]\nb1, b2, b3, b4 = best[\"cat_w_effnet\"], best[\"cat_w_resnet\"], best[\"cat_w_vit\"], best[\"cat_w_tab\"]\nc1, c2, c3, c4 = best[\"lr_w_effnet\"], best[\"lr_w_resnet\"], best[\"lr_w_vit\"], best[\"lr_w_tab\"]\n\nX_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [a1, a2, a3, a4])\nX_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [a1, a2, a3, a4])\n\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [b1, b2, b3, b4])\nX_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [b1, b2, b3, b4])\n\nX_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [c1, c2, c3, c4])\nX_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [c1, c2, c3, c4])\n\n# Initialize final models with best parameters (adjust if optimized hyperparams are included)\nfinal_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    n_jobs=-1,\n)\n\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    random_seed=42,\n    verbose=False,\n)\n\nfinal_lr_base = LogisticRegression(\n    C=best[\"lr_base_C\"],\n    solver=best[\"lr_base_solver\"],\n    max_iter=1000,\n    n_jobs=None if best[\"lr_base_solver\"] == \"liblinear\" else -1,\n)\n\n# Get OOF predictions again for full dataset\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr, test_lr = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test = np.hstack([test_xgb, test_cat, test_lr])\n\n# Final meta learner\nfinal_meta = LogisticRegression(\n    C=best[\"lr_meta_C\"],\n    solver=best[\"lr_meta_solver\"],\n    max_iter=1000,\n    n_jobs=None if best[\"lr_meta_solver\"] == \"liblinear\" else -1,\n)\nfinal_meta.fit(meta_train, y_train_)\n\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\nfinal_preds = (final_probs >= 0.3).astype(int)\n\nprint(\"\\nFinal AUC:\", roc_auc_score(y_test_, final_probs))\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T07:46:21.183591Z","iopub.execute_input":"2025-08-16T07:46:21.184227Z","iopub.status.idle":"2025-08-16T08:10:58.901425Z","shell.execute_reply.started":"2025-08-16T07:46:21.184203Z","shell.execute_reply":"2025-08-16T08:10:58.900423Z"}},"outputs":[{"name":"stderr","text":"[I 2025-08-16 07:46:21,201] A new study created in memory with name: no-name-923219ff-cf8f-4682-a391-f76e7a5ed81b\n[I 2025-08-16 07:47:09,728] Trial 0 finished with value: 0.6016949152542372 and parameters: {'xgb_w_effnet': 3.926140141085941, 'xgb_w_resnet': 2.7285790624334183, 'xgb_w_vit': 2.746697398490894, 'xgb_w_tab': 2.359875852910637, 'cat_w_effnet': 2.7944499838754497, 'cat_w_resnet': 2.808473109664984, 'cat_w_vit': 2.241863009681797, 'cat_w_tab': 4.504605891044994, 'lr_w_effnet': 2.186859979249937, 'lr_w_resnet': 2.909168529014704, 'lr_w_vit': 3.4543542050629297, 'lr_w_tab': 4.6309386003629704, 'lr_base_C': 0.28182211872118657, 'lr_base_solver': 'lbfgs', 'lr_meta_C': 0.5212869178111094, 'lr_meta_solver': 'saga'}. Best is trial 0 with value: 0.6016949152542372.\n[I 2025-08-16 07:49:07,013] Trial 1 finished with value: 0.6016949152542372 and parameters: {'xgb_w_effnet': 2.570548955404649, 'xgb_w_resnet': 1.214235038762013, 'xgb_w_vit': 2.6935393522142337, 'xgb_w_tab': 3.9540748451804584, 'cat_w_effnet': 4.560289904197639, 'cat_w_resnet': 1.2340218780532917, 'cat_w_vit': 2.8005902761478896, 'cat_w_tab': 2.4436042163746685, 'lr_w_effnet': 1.4600447310380646, 'lr_w_resnet': 1.163287009779082, 'lr_w_vit': 4.299114645531825, 'lr_w_tab': 2.8575126521872245, 'lr_base_C': 1.4095422674082494, 'lr_base_solver': 'saga', 'lr_meta_C': 0.6459673030966578, 'lr_meta_solver': 'saga'}. Best is trial 0 with value: 0.6016949152542372.\n[I 2025-08-16 07:51:04,393] Trial 2 finished with value: 0.652542372881356 and parameters: {'xgb_w_effnet': 4.766746708928198, 'xgb_w_resnet': 1.356843673559308, 'xgb_w_vit': 4.354880075997494, 'xgb_w_tab': 2.866253185264432, 'cat_w_effnet': 3.4924545852421613, 'cat_w_resnet': 3.3711621718407794, 'cat_w_vit': 3.5175230773287547, 'cat_w_tab': 3.4046630760348555, 'lr_w_effnet': 2.6276246698515515, 'lr_w_resnet': 2.421858421259117, 'lr_w_vit': 1.9904708272885103, 'lr_w_tab': 1.7758187069228106, 'lr_base_C': 4.27385698904425, 'lr_base_solver': 'saga', 'lr_meta_C': 0.13966705940844326, 'lr_meta_solver': 'saga'}. Best is trial 2 with value: 0.652542372881356.\n[I 2025-08-16 07:53:03,597] Trial 3 finished with value: 0.6186440677966102 and parameters: {'xgb_w_effnet': 4.029699209130621, 'xgb_w_resnet': 1.018657727837732, 'xgb_w_vit': 2.0593637794726747, 'xgb_w_tab': 2.253869295109749, 'cat_w_effnet': 2.5431849190995535, 'cat_w_resnet': 2.2415302835251527, 'cat_w_vit': 3.638799018067012, 'cat_w_tab': 1.77495352515199, 'lr_w_effnet': 3.434975212269761, 'lr_w_resnet': 3.432602756063418, 'lr_w_vit': 3.0031253750686036, 'lr_w_tab': 4.18441835710042, 'lr_base_C': 0.41434224814773274, 'lr_base_solver': 'saga', 'lr_meta_C': 0.5384201214167361, 'lr_meta_solver': 'lbfgs'}. Best is trial 2 with value: 0.652542372881356.\n[I 2025-08-16 07:55:03,360] Trial 4 finished with value: 0.7033898305084746 and parameters: {'xgb_w_effnet': 1.2744163525389447, 'xgb_w_resnet': 3.8506899664461667, 'xgb_w_vit': 4.633443604459609, 'xgb_w_tab': 2.0040074578951814, 'cat_w_effnet': 2.7494592319375117, 'cat_w_resnet': 3.919304934832691, 'cat_w_vit': 3.856184549662429, 'cat_w_tab': 1.8022317160133112, 'lr_w_effnet': 1.5525237132244576, 'lr_w_resnet': 2.9614071379430094, 'lr_w_vit': 2.6977804141621013, 'lr_w_tab': 3.9349363689613215, 'lr_base_C': 0.20827870576776278, 'lr_base_solver': 'saga', 'lr_meta_C': 0.02102199662078099, 'lr_meta_solver': 'liblinear'}. Best is trial 4 with value: 0.7033898305084746.\n[I 2025-08-16 07:57:01,281] Trial 5 finished with value: 0.7457627118644068 and parameters: {'xgb_w_effnet': 2.1090415759421113, 'xgb_w_resnet': 1.055691662623111, 'xgb_w_vit': 1.1126565067671352, 'xgb_w_tab': 3.8266293398994535, 'cat_w_effnet': 1.9247629650640974, 'cat_w_resnet': 2.1713403583217192, 'cat_w_vit': 4.56173316449531, 'cat_w_tab': 4.407439095819022, 'lr_w_effnet': 4.530063805726909, 'lr_w_resnet': 1.0035828405756382, 'lr_w_vit': 4.432094235491238, 'lr_w_tab': 2.581771255175537, 'lr_base_C': 1.6485387846342128, 'lr_base_solver': 'saga', 'lr_meta_C': 0.006844480402908362, 'lr_meta_solver': 'liblinear'}. Best is trial 5 with value: 0.7457627118644068.\n[I 2025-08-16 07:59:00,536] Trial 6 finished with value: 0.5932203389830508 and parameters: {'xgb_w_effnet': 4.002543657514014, 'xgb_w_resnet': 2.59952592929136, 'xgb_w_vit': 1.9330849878911387, 'xgb_w_tab': 2.8746889414646226, 'cat_w_effnet': 4.57673128671863, 'cat_w_resnet': 3.1428869679577245, 'cat_w_vit': 3.5575248085132505, 'cat_w_tab': 2.2818561185162265, 'lr_w_effnet': 3.7017917235273337, 'lr_w_resnet': 2.2651465691009225, 'lr_w_vit': 3.7802884790157605, 'lr_w_tab': 3.2375617046320433, 'lr_base_C': 0.1886406474915355, 'lr_base_solver': 'saga', 'lr_meta_C': 1.2035194482271705, 'lr_meta_solver': 'lbfgs'}. Best is trial 5 with value: 0.7457627118644068.\n[I 2025-08-16 07:59:44,656] Trial 7 finished with value: 0.6779661016949152 and parameters: {'xgb_w_effnet': 4.52588028497844, 'xgb_w_resnet': 4.405010356177474, 'xgb_w_vit': 2.2063999693841136, 'xgb_w_tab': 2.4330056643998814, 'cat_w_effnet': 4.173001622142554, 'cat_w_resnet': 3.814503338215979, 'cat_w_vit': 2.3831976371684416, 'cat_w_tab': 3.219900478250537, 'lr_w_effnet': 3.748557195787053, 'lr_w_resnet': 3.1536503404626144, 'lr_w_vit': 4.687752790881862, 'lr_w_tab': 4.547193843037466, 'lr_base_C': 0.0076928962372466774, 'lr_base_solver': 'lbfgs', 'lr_meta_C': 0.045558486804822884, 'lr_meta_solver': 'saga'}. Best is trial 5 with value: 0.7457627118644068.\n[I 2025-08-16 08:00:31,997] Trial 8 finished with value: 0.5677966101694916 and parameters: {'xgb_w_effnet': 2.5963810093026116, 'xgb_w_resnet': 3.422282398352162, 'xgb_w_vit': 3.7510788544344438, 'xgb_w_tab': 2.991067940796262, 'cat_w_effnet': 4.905910439577683, 'cat_w_resnet': 1.9567060764757414, 'cat_w_vit': 2.2463542582031253, 'cat_w_tab': 1.5165861304478288, 'lr_w_effnet': 2.7985658533504085, 'lr_w_resnet': 2.3324114594897716, 'lr_w_vit': 3.4695750563815717, 'lr_w_tab': 1.0266768448260817, 'lr_base_C': 0.007687489594647988, 'lr_base_solver': 'saga', 'lr_meta_C': 5.7335839217778455, 'lr_meta_solver': 'lbfgs'}. Best is trial 5 with value: 0.7457627118644068.\n[I 2025-08-16 08:02:29,347] Trial 9 finished with value: 0.5677966101694916 and parameters: {'xgb_w_effnet': 3.399128538709018, 'xgb_w_resnet': 3.658992609657399, 'xgb_w_vit': 1.6538364506668577, 'xgb_w_tab': 2.418879805276997, 'cat_w_effnet': 3.52837200838885, 'cat_w_resnet': 1.8671448936006274, 'cat_w_vit': 4.549224405532035, 'cat_w_tab': 3.343431697618957, 'lr_w_effnet': 1.298937220847336, 'lr_w_resnet': 4.667305875853849, 'lr_w_vit': 3.47998209119418, 'lr_w_tab': 4.7124023748267225, 'lr_base_C': 1.086661124769912, 'lr_base_solver': 'saga', 'lr_meta_C': 5.307790892801796, 'lr_meta_solver': 'liblinear'}. Best is trial 5 with value: 0.7457627118644068.\n[I 2025-08-16 08:03:15,829] Trial 10 finished with value: 1.0 and parameters: {'xgb_w_effnet': 1.4888826533510902, 'xgb_w_resnet': 2.165285934146347, 'xgb_w_vit': 1.0466640567729575, 'xgb_w_tab': 4.777546598473762, 'cat_w_effnet': 1.0720426982785107, 'cat_w_resnet': 4.7466110146072245, 'cat_w_vit': 1.472749754075691, 'cat_w_tab': 4.831277153944761, 'lr_w_effnet': 4.795690791296725, 'lr_w_resnet': 1.0902868320572001, 'lr_w_vit': 1.6985034089390503, 'lr_w_tab': 2.261680719595783, 'lr_base_C': 9.539611095464293, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.001313628816632507, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:04:04,705] Trial 11 finished with value: 1.0 and parameters: {'xgb_w_effnet': 1.4073961262128758, 'xgb_w_resnet': 2.0018430247525068, 'xgb_w_vit': 1.0220337002412763, 'xgb_w_tab': 4.520865583096642, 'cat_w_effnet': 1.110066891923189, 'cat_w_resnet': 4.784155589047498, 'cat_w_vit': 1.2670425687204285, 'cat_w_tab': 4.986460098751952, 'lr_w_effnet': 4.945021283743003, 'lr_w_resnet': 1.055092467917545, 'lr_w_vit': 1.0775237860317066, 'lr_w_tab': 2.4148979998432214, 'lr_base_C': 9.437837561266074, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.0010370110328198176, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:04:54,174] Trial 12 finished with value: 1.0 and parameters: {'xgb_w_effnet': 1.021322605857998, 'xgb_w_resnet': 2.150640635508833, 'xgb_w_vit': 1.0499528992348006, 'xgb_w_tab': 4.91390695325108, 'cat_w_effnet': 1.1176576464508345, 'cat_w_resnet': 4.846997976498584, 'cat_w_vit': 1.0969282140025312, 'cat_w_tab': 4.982022263419052, 'lr_w_effnet': 4.967151244252391, 'lr_w_resnet': 1.6219002772478834, 'lr_w_vit': 1.0247848577942715, 'lr_w_tab': 2.1044898835567927, 'lr_base_C': 4.419721478649939, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.0011163819699149078, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:05:42,737] Trial 13 finished with value: 1.0 and parameters: {'xgb_w_effnet': 1.5969578841030123, 'xgb_w_resnet': 1.966784564861143, 'xgb_w_vit': 1.4946759650273385, 'xgb_w_tab': 4.97646920748282, 'cat_w_effnet': 1.0806139390435214, 'cat_w_resnet': 4.874270784853148, 'cat_w_vit': 1.0036840759053587, 'cat_w_tab': 4.028276279824404, 'lr_w_effnet': 4.303987075763778, 'lr_w_resnet': 1.6085785179964827, 'lr_w_vit': 1.009997346283014, 'lr_w_tab': 1.8222134866514033, 'lr_base_C': 8.200448387465647, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.0011729107980658969, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:06:24,934] Trial 14 finished with value: 0.8050847457627118 and parameters: {'xgb_w_effnet': 1.7331561744250479, 'xgb_w_resnet': 1.9326823244907394, 'xgb_w_vit': 3.4763263231937636, 'xgb_w_tab': 4.2296121105090565, 'cat_w_effnet': 1.6902828025432388, 'cat_w_resnet': 4.320616099846135, 'cat_w_vit': 1.6239350940173336, 'cat_w_tab': 3.969865861577566, 'lr_w_effnet': 4.946334671351695, 'lr_w_resnet': 1.6460402042432802, 'lr_w_vit': 1.6857050255902633, 'lr_w_tab': 3.4538273378641593, 'lr_base_C': 0.028378712154222645, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.003373775445909113, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:07:05,670] Trial 15 finished with value: 0.7203389830508474 and parameters: {'xgb_w_effnet': 2.066124493784444, 'xgb_w_resnet': 2.3009251108465407, 'xgb_w_vit': 1.378757461742116, 'xgb_w_tab': 4.518364171823303, 'cat_w_effnet': 1.827403110039306, 'cat_w_resnet': 4.369061571022422, 'cat_w_vit': 1.626752260536139, 'cat_w_tab': 4.97029727758655, 'lr_w_effnet': 4.228987224165792, 'lr_w_resnet': 3.8331617982793915, 'lr_w_vit': 1.9229247618960628, 'lr_w_tab': 2.3361428625841376, 'lr_base_C': 0.0011587732224502818, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.008947999638564604, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:07:50,018] Trial 16 finished with value: 0.788135593220339 and parameters: {'xgb_w_effnet': 3.0167451146989634, 'xgb_w_resnet': 3.0902868622019093, 'xgb_w_vit': 2.308297646094281, 'xgb_w_tab': 1.0715412565755038, 'cat_w_effnet': 1.4578907959615115, 'cat_w_resnet': 4.406617509575396, 'cat_w_vit': 1.5508970407079867, 'cat_w_tab': 3.884231716374087, 'lr_w_effnet': 4.435458614212536, 'lr_w_resnet': 1.8619291448670328, 'lr_w_vit': 1.5647945343821021, 'lr_w_tab': 1.2731386945264886, 'lr_base_C': 9.298804997195974, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.003733882200252675, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:08:34,540] Trial 17 finished with value: 0.9830508474576272 and parameters: {'xgb_w_effnet': 1.603446948702358, 'xgb_w_resnet': 1.6854640727562993, 'xgb_w_vit': 3.2396140232461015, 'xgb_w_tab': 3.5583095506757116, 'cat_w_effnet': 2.266204476659163, 'cat_w_resnet': 3.6345172664983707, 'cat_w_vit': 1.8771027629304868, 'cat_w_tab': 4.677698805089651, 'lr_w_effnet': 3.419803444385259, 'lr_w_resnet': 1.0026102542087099, 'lr_w_vit': 2.311453601585881, 'lr_w_tab': 2.8263884055457833, 'lr_base_C': 0.04112417680471182, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.0021213638097143544, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:09:19,773] Trial 18 finished with value: 0.711864406779661 and parameters: {'xgb_w_effnet': 2.2603594199702197, 'xgb_w_resnet': 1.6534782885626886, 'xgb_w_vit': 1.0350319802757548, 'xgb_w_tab': 4.538182699371821, 'cat_w_effnet': 2.125787245311254, 'cat_w_resnet': 4.874922178928529, 'cat_w_vit': 2.797320448281766, 'cat_w_tab': 2.6324005462972244, 'lr_w_effnet': 3.9900042196526364, 'lr_w_resnet': 4.349716756808096, 'lr_w_vit': 1.4156027520438306, 'lr_w_tab': 1.5453529490430973, 'lr_base_C': 0.761893469632016, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.016267873701255264, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:10:06,193] Trial 19 finished with value: 0.6694915254237288 and parameters: {'xgb_w_effnet': 1.1487362828931076, 'xgb_w_resnet': 2.6987369735726605, 'xgb_w_vit': 1.7255464786055792, 'xgb_w_tab': 3.456366067761431, 'cat_w_effnet': 1.392297181693156, 'cat_w_resnet': 4.0463608542600475, 'cat_w_vit': 1.2440513456141584, 'cat_w_tab': 1.000982807792386, 'lr_w_effnet': 4.739339508483684, 'lr_w_resnet': 1.959368567016924, 'lr_w_vit': 2.39241359764854, 'lr_w_tab': 2.2772750924306653, 'lr_base_C': 3.023655533902883, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.08687171612081104, 'lr_meta_solver': 'lbfgs'}. Best is trial 10 with value: 1.0.\n[I 2025-08-16 08:10:56,070] Trial 20 finished with value: 0.7372881355932204 and parameters: {'xgb_w_effnet': 3.033509951150923, 'xgb_w_resnet': 4.966164230627019, 'xgb_w_vit': 4.035937286490232, 'xgb_w_tab': 4.421762917638493, 'cat_w_effnet': 3.2461231240355035, 'cat_w_resnet': 2.701373060317888, 'cat_w_vit': 1.9604910938974962, 'cat_w_tab': 4.227148510136859, 'lr_w_effnet': 2.2209626459690743, 'lr_w_resnet': 1.3960654991631403, 'lr_w_vit': 1.2995440806733978, 'lr_w_tab': 3.571900100248673, 'lr_base_C': 2.764004169163996, 'lr_base_solver': 'lbfgs', 'lr_meta_C': 0.007309902410687457, 'lr_meta_solver': 'liblinear'}. Best is trial 10 with value: 1.0.\n[W 2025-08-16 08:10:58,820] Trial 21 failed with parameters: {'xgb_w_effnet': 1.0888749920198886, 'xgb_w_resnet': 2.2879882178456796, 'xgb_w_vit': 1.1212934764089395, 'xgb_w_tab': 4.872304763590743, 'cat_w_effnet': 1.0352534128803428, 'cat_w_resnet': 4.91075669936598, 'cat_w_vit': 1.0793893315944028, 'cat_w_tab': 4.981541218882625, 'lr_w_effnet': 4.9719540749135565, 'lr_w_resnet': 1.388945210922878, 'lr_w_vit': 1.0432266752499588, 'lr_w_tab': 2.1134720717093707, 'lr_base_C': 7.84368109710949, 'lr_base_solver': 'liblinear', 'lr_meta_C': 0.0010765578108546934, 'lr_meta_solver': 'liblinear'} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_629/2686448198.py\", line 108, in objective\n    oof_xgb, test_xgb = get_oof_preds(base_xgb, X_xgb_train, y_train_, X_xgb_test)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_629/2686448198.py\", line 34, in get_oof_preds\n    model.fit(X_tr, y_tr)\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1519, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 181, in train\n    bst.update(dtrain, i, obj)\n  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2051, in update\n    _LIB.XGBoosterUpdateOneIter(\nKeyboardInterrupt\n[W 2025-08-16 08:10:58,829] Trial 21 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_629/2686448198.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# Run Optuna optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best recall:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_629/2686448198.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Get out-of-fold predictions and test predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0moof_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_xgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_xgb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0moof_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cat_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0moof_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lr_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_629/2686448198.py\u001b[0m in \u001b[0;36mget_oof_preds\u001b[0;34m(model, X, y, X_test, n_splits, random_state)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mX_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_va\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtest_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":91},{"cell_type":"code","source":"# Rebuild weighted feature sets using Optuna's best parameters\na1, a2, a3, a4 = 1.733, 1.932, 3.476, 4.229\nb1, b2, b3, b4 = 1.690, 4.320, 1.623, 3.969\nc1, c2, c3, c4 = 4.946, 1.646, 1.685, 3.453\n\nX_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [a1, a2, a3, a4])\nX_xgb_test  = weighted_concat([X_test_effnet,  X_test_resnet,  X_test_vit_im,  X_test_tab_scaled],  [a1, a2, a3, a4])\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [b1, b2, b3, b4])\nX_cat_test  = weighted_concat([X_test_effnet,  X_test_resnet,  X_test_vit_im,  X_test_tab_scaled],  [b1, b2, b3, b4])\nX_lr_train  = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [c1, c2, c3, c4])\nX_lr_test   = weighted_concat([X_test_effnet,  X_test_resnet,  X_test_vit_im,  X_test_tab_scaled],  [c1, c2, c3, c4])\n\n# Initialize final models with best parameters (from study)\nfinal_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    n_jobs=-1,\n)\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    random_seed=42,\n    verbose=False,\n)\nfinal_lr_base = LogisticRegression(\n    C=0.028378,\n    solver='liblinear',\n    max_iter=1000,\n    n_jobs=None if 'liblinear' == \"liblinear\" else -1,\n)\n\n# Get OOF predictions for stacking\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr,  test_lr  = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test  = np.hstack([test_xgb, test_cat, test_lr])\n\n# Final meta learner\nfinal_meta = LogisticRegression(\n    C=best[\"lr_meta_C\"],\n    solver=best[\"lr_meta_solver\"],\n    max_iter=1000,\n    n_jobs=None if best[\"lr_meta_solver\"] == \"liblinear\" else -1,\n)\nfinal_meta.fit(meta_train, y_train_)\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\n\n# --- Find best threshold for recall or F1 ---\nimport numpy as np\nfrom sklearn.metrics import recall_score, f1_score\n\nthresholds = np.arange(0.05, 0.95, 0.01)\nrecalls = []\nf1s = []\n\nfor t in thresholds:\n    preds = (final_probs >= t).astype(int)\n    recalls.append(recall_score(y_test_, preds, pos_label=1))\n    f1s.append(f1_score(y_test_, preds, pos_label=1))\n\nbest_recall_idx = np.argmax(recalls)\nbest_f1_idx = np.argmax(f1s)\nbest_recall_threshold = thresholds[best_recall_idx]\nbest_f1_threshold = thresholds[best_f1_idx]\n\nprint(f'Best recall: {recalls[best_recall_idx]:.4f} at threshold={best_recall_threshold:.2f}')\nprint(f'Best F1: {f1s[best_f1_idx]:.4f} at threshold={best_f1_threshold:.2f}')\n\n# Classification report at recall-optimal threshold:\noptimal_preds = (final_probs >= best_recall_threshold).astype(int)\nprint(\"\\nClassification report (recall-optimal threshold):\")\nprint(classification_report(y_test_, optimal_preds, digits=4))\n\nprint(f\"\\nFinal AUC: {roc_auc_score(y_test_, final_probs):.4f}\")\n\n# You can also print the report at any threshold, e.g., 0.3:\npreds_03 = (final_probs >= 0.3).astype(int)\nprint(\"\\nClassification report (threshold=0.3):\")\nprint(classification_report(y_test_, preds_03, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:13:14.787578Z","iopub.execute_input":"2025-08-16T08:13:14.788161Z","iopub.status.idle":"2025-08-16T08:14:10.467789Z","shell.execute_reply.started":"2025-08-16T08:13:14.788134Z","shell.execute_reply":"2025-08-16T08:14:10.466989Z"}},"outputs":[{"name":"stdout","text":"Best recall: 0.7373 at threshold=0.05\nBest F1: 0.6957 at threshold=0.22\n\nClassification report (recall-optimal threshold):\n              precision    recall  f1-score   support\n\n         0.0     0.9590    0.9052    0.9314       802\n         1.0     0.5337    0.7373    0.6192       118\n\n    accuracy                         0.8837       920\n   macro avg     0.7464    0.8213    0.7753       920\nweighted avg     0.9045    0.8837    0.8913       920\n\n\nFinal AUC: 0.9204\n\nClassification report (threshold=0.3):\n              precision    recall  f1-score   support\n\n         0.0     0.9496    0.9626    0.9560       802\n         1.0     0.7196    0.6525    0.6844       118\n\n    accuracy                         0.9228       920\n   macro avg     0.8346    0.8076    0.8202       920\nweighted avg     0.9201    0.9228    0.9212       920\n\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"print(\"Best recall:\", study.best_value)\nprint(\"Best parameters:\")\nfor key, value in study.best_params.items():\n    print(f\"  {key}: {value}\")\n\n# Train final model with best parameters\nbest = study.best_params\n\n# Reconstruct weighted feature sets with best weights\na1, a2, a3, a4 = best[\"xgb_w_effnet\"], best[\"xgb_w_resnet\"], best[\"xgb_w_vit\"], best[\"xgb_w_tab\"]\nb1, b2, b3, b4 = best[\"cat_w_effnet\"], best[\"cat_w_resnet\"], best[\"cat_w_vit\"], best[\"cat_w_tab\"]\nc1, c2, c3, c4 = best[\"lr_w_effnet\"], best[\"lr_w_resnet\"], best[\"lr_w_vit\"], best[\"lr_w_tab\"]\n\nX_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [a1, a2, a3, a4])\nX_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [a1, a2, a3, a4])\n\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [b1, b2, b3, b4])\nX_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [b1, b2, b3, b4])\n\nX_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled], [c1, c2, c3, c4])\nX_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled], [c1, c2, c3, c4])\n\n# Initialize final models with best parameters (adjust if optimized hyperparams are included)\nfinal_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    n_jobs=-1,\n)\n\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    random_seed=42,\n    verbose=False,\n)\n\nfinal_lr_base = LogisticRegression(\n    C=best[\"lr_base_C\"],\n    solver=best[\"lr_base_solver\"],\n    max_iter=1000,\n    n_jobs=None if best[\"lr_base_solver\"] == \"liblinear\" else -1,\n)\n\n# Get OOF predictions again for full dataset\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr, test_lr = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test = np.hstack([test_xgb, test_cat, test_lr])\n\n# Final meta learner\nfinal_meta = LogisticRegression(\n    C=best[\"lr_meta_C\"],\n    solver=best[\"lr_meta_solver\"],\n    max_iter=1000,\n    n_jobs=None if best[\"lr_meta_solver\"] == \"liblinear\" else -1,\n)\nfinal_meta.fit(meta_train, y_train_)\n\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\nfinal_preds = (final_probs >= 0.3).astype(int)\n\nprint(\"\\nFinal AUC:\", roc_auc_score(y_test_, final_probs))\nprint(classification_report(y_test_, final_preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:15:32.752468Z","iopub.execute_input":"2025-08-16T08:15:32.753134Z","iopub.status.idle":"2025-08-16T08:16:19.640613Z","shell.execute_reply.started":"2025-08-16T08:15:32.753104Z","shell.execute_reply":"2025-08-16T08:16:19.639824Z"}},"outputs":[{"name":"stdout","text":"Best recall: 1.0\nBest parameters:\n  xgb_w_effnet: 1.4888826533510902\n  xgb_w_resnet: 2.165285934146347\n  xgb_w_vit: 1.0466640567729575\n  xgb_w_tab: 4.777546598473762\n  cat_w_effnet: 1.0720426982785107\n  cat_w_resnet: 4.7466110146072245\n  cat_w_vit: 1.472749754075691\n  cat_w_tab: 4.831277153944761\n  lr_w_effnet: 4.795690791296725\n  lr_w_resnet: 1.0902868320572001\n  lr_w_vit: 1.6985034089390503\n  lr_w_tab: 2.261680719595783\n  lr_base_C: 9.539611095464293\n  lr_base_solver: liblinear\n  lr_meta_C: 0.001313628816632507\n  lr_meta_solver: liblinear\n\nFinal AUC: 0.9086077179931528\n              precision    recall  f1-score   support\n\n         0.0     0.0000    0.0000    0.0000       802\n         1.0     0.1283    1.0000    0.2274       118\n\n    accuracy                         0.1283       920\n   macro avg     0.0641    0.5000    0.1137       920\nweighted avg     0.0165    0.1283    0.0292       920\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score\n\nfinal_meta.fit(meta_train, y_train_)\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\nfinal_preds = (final_probs >= 0.3).astype(int)  # Apply threshold\n\nprint(\"\\nFinal AUC:\", roc_auc_score(y_test_, final_probs))\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:16:31.974349Z","iopub.execute_input":"2025-08-16T08:16:31.974656Z","iopub.status.idle":"2025-08-16T08:16:31.994300Z","shell.execute_reply.started":"2025-08-16T08:16:31.974635Z","shell.execute_reply":"2025-08-16T08:16:31.993459Z"}},"outputs":[{"name":"stdout","text":"\nFinal AUC: 0.9086077179931528\n              precision    recall  f1-score   support\n\n         0.0     0.0000    0.0000    0.0000       802\n         1.0     0.1283    1.0000    0.2274       118\n\n    accuracy                         0.1283       920\n   macro avg     0.0641    0.5000    0.1137       920\nweighted avg     0.0165    0.1283    0.0292       920\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score\n\n# Set best parameters from Optuna Trial 14\nbest = {\n    'xgb_w_effnet': 1.7331561744250479,\n    'xgb_w_resnet': 1.9326823244907394,\n    'xgb_w_vit': 3.4763263231937636,\n    'xgb_w_tab': 4.2296121105090565,\n    'cat_w_effnet': 1.6902828025432388,\n    'cat_w_resnet': 4.320616099846135,\n    'cat_w_vit': 1.6239350940173336,\n    'cat_w_tab': 3.969865861577566,\n    'lr_w_effnet': 4.946334671351695,\n    'lr_w_resnet': 1.6460402042432802,\n    'lr_w_vit': 1.6857050255902633,\n    'lr_w_tab': 3.4538273378641593,\n    'lr_base_C': 0.028378712154222645,\n    'lr_base_solver': 'liblinear',\n    'lr_meta_C': 0.003373775445909113,\n    'lr_meta_solver': 'liblinear'\n}\n\n# Rebuild weighted feature arrays for base learners using weighted_concat function\nX_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                             [best['xgb_w_effnet'], best['xgb_w_resnet'], best['xgb_w_vit'], best['xgb_w_tab']])\nX_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                            [best['xgb_w_effnet'], best['xgb_w_resnet'], best['xgb_w_vit'], best['xgb_w_tab']])\n\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                              [best['cat_w_effnet'], best['cat_w_resnet'], best['cat_w_vit'], best['cat_w_tab']])\nX_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                             [best['cat_w_effnet'], best['cat_w_resnet'], best['cat_w_vit'], best['cat_w_tab']])\n\nX_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                            [best['lr_w_effnet'], best['lr_w_resnet'], best['lr_w_vit'], best['lr_w_tab']])\nX_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                           [best['lr_w_effnet'], best['lr_w_resnet'], best['lr_w_vit'], best['lr_w_tab']])\n\n# Define base learners with fixed params + weighted features\nfinal_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    n_jobs=-1,\n)\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    random_seed=42,\n    verbose=False,\n)\nfinal_lr_base = LogisticRegression(\n    C=best['lr_base_C'],\n    solver=best['lr_base_solver'],\n    max_iter=1000,\n    n_jobs=None if best['lr_base_solver'] == 'liblinear' else -1,\n)\n\n# Get OOF predictions for stacking meta features\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr, test_lr = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test = np.hstack([test_xgb, test_cat, test_lr])\n\n# Final meta learner (logistic regression)\nfinal_meta = LogisticRegression(\n    C=best['lr_meta_C'],\n    solver=best['lr_meta_solver'],\n    max_iter=1000,\n    n_jobs=None if best['lr_meta_solver'] == 'liblinear' else -1,\n)\nfinal_meta.fit(meta_train, y_train_)\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\n\n# Threshold predictions at 0.3\nfinal_preds = (final_probs >= 0.3).astype(int)\n\n# Print metrics\nprint(\"\\nFinal ROC AUC:\", roc_auc_score(y_test_, final_probs))\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:23:28.307869Z","iopub.execute_input":"2025-08-16T08:23:28.308285Z","iopub.status.idle":"2025-08-16T08:24:10.443787Z","shell.execute_reply.started":"2025-08-16T08:23:28.308257Z","shell.execute_reply":"2025-08-16T08:24:10.442982Z"}},"outputs":[{"name":"stdout","text":"\nFinal ROC AUC: 0.9094530622596052\n              precision    recall  f1-score   support\n\n         0.0     0.9663    0.8217    0.8881       802\n         1.0     0.3992    0.8051    0.5337       118\n\n    accuracy                         0.8196       920\n   macro avg     0.6827    0.8134    0.7109       920\nweighted avg     0.8935    0.8196    0.8427       920\n\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score\n\n# Use Trial 16 best parameters\nbest = {\n    'xgb_w_effnet': 3.0167451146989634,\n    'xgb_w_resnet': 3.0902868622019093,\n    'xgb_w_vit': 2.308297646094281,\n    'xgb_w_tab': 1.0715412565755038,\n    'cat_w_effnet': 1.4578907959615115,\n    'cat_w_resnet': 4.406617509575396,\n    'cat_w_vit': 1.5508970407079867,\n    'cat_w_tab': 3.884231716374087,\n    'lr_w_effnet': 4.435458614212536,\n    'lr_w_resnet': 1.8619291448670328,\n    'lr_w_vit': 1.5647945343821021,\n    'lr_w_tab': 1.2731386945264886,\n    'lr_base_C': 9.298804997195974,\n    'lr_base_solver': 'liblinear',\n    'lr_meta_C': 0.003733882200252675,\n    'lr_meta_solver': 'liblinear'\n}\n\n# Rebuild weighted data\nX_xgb_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                             [best['xgb_w_effnet'], best['xgb_w_resnet'], best['xgb_w_vit'], best['xgb_w_tab']])\nX_xgb_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                            [best['xgb_w_effnet'], best['xgb_w_resnet'], best['xgb_w_vit'], best['xgb_w_tab']])\n\nX_cat_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                              [best['cat_w_effnet'], best['cat_w_resnet'], best['cat_w_vit'], best['cat_w_tab']])\nX_cat_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                             [best['cat_w_effnet'], best['cat_w_resnet'], best['cat_w_vit'], best['cat_w_tab']])\n\nX_lr_train = weighted_concat([X_train_effnet, X_train_resnet, X_train_vit_im, X_train_tab_scaled],\n                            [best['lr_w_effnet'], best['lr_w_resnet'], best['lr_w_vit'], best['lr_w_tab']])\nX_lr_test = weighted_concat([X_test_effnet, X_test_resnet, X_test_vit_im, X_test_tab_scaled],\n                           [best['lr_w_effnet'], best['lr_w_resnet'], best['lr_w_vit'], best['lr_w_tab']])\n\n# Initialize final models with trial 16 parameters\nfinal_xgb = XGBClassifier(\n    n_estimators=300,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.85,\n    colsample_bytree=0.7,\n    reg_lambda=2e-3,\n    random_state=42,\n    eval_metric='logloss',\n    use_label_encoder=False,\n    n_jobs=-1,\n)\nfinal_cat = CatBoostClassifier(\n    iterations=500,\n    depth=5,\n    learning_rate=2e-3,\n    l2_leaf_reg=3,\n    loss_function='Logloss',\n    eval_metric='AUC',\n    random_seed=42,\n    verbose=False,\n)\nfinal_lr_base = LogisticRegression(\n    C=best['lr_base_C'],\n    solver=best['lr_base_solver'],\n    max_iter=1000,\n    n_jobs=None if best['lr_base_solver'] == 'liblinear' else -1,\n)\n\n# Get OOF preds for stacking\noof_xgb, test_xgb = get_oof_preds(final_xgb, X_xgb_train, y_train_, X_xgb_test)\noof_cat, test_cat = get_oof_preds(final_cat, X_cat_train, y_train_, X_cat_test)\noof_lr, test_lr = get_oof_preds(final_lr_base, X_lr_train, y_train_, X_lr_test)\n\nmeta_train = np.hstack([oof_xgb, oof_cat, oof_lr])\nmeta_test = np.hstack([test_xgb, test_cat, test_lr])\n\n# Final meta learner setup and fit\nfinal_meta = LogisticRegression(\n    C=best['lr_meta_C'],\n    solver=best['lr_meta_solver'],\n    max_iter=1000,\n    n_jobs=None if best['lr_meta_solver'] == 'liblinear' else -1,\n)\nfinal_meta.fit(meta_train, y_train_)\n\n# Predict final probabilities and apply threshold 0.3\nfinal_probs = final_meta.predict_proba(meta_test)[:, 1]\nfinal_preds = (final_probs >= 0.3).astype(int)\n\n# Evaluation output\nprint(\"\\nFinal AUC:\", roc_auc_score(y_test_, final_probs))\nprint(classification_report(y_test_, final_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:27:08.108398Z","iopub.execute_input":"2025-08-16T08:27:08.109094Z","iopub.status.idle":"2025-08-16T08:27:54.798643Z","shell.execute_reply.started":"2025-08-16T08:27:08.109066Z","shell.execute_reply":"2025-08-16T08:27:54.797699Z"}},"outputs":[{"name":"stdout","text":"\nFinal AUC: 0.9099285684094848\n              precision    recall  f1-score   support\n\n         0.0     0.9644    0.8441    0.9003       802\n         1.0     0.4266    0.7881    0.5536       118\n\n    accuracy                         0.8370       920\n   macro avg     0.6955    0.8161    0.7269       920\nweighted avg     0.8954    0.8370    0.8558       920\n\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}